{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"invoke-training","text":"<p>A library for training custom Stable Diffusion models (fine-tuning, LoRA training, textual inversion, etc.) that can be used in InvokeAI.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is organized as follows:</p> <ul> <li>Get Started: Install <code>invoke-training</code> and run your first training pipeline.</li> <li>Guides: Full tutorials for running popular training pipelines.</li> <li>Config Reference: Reference documentation for all supported training configuration options.</li> <li>Contributing: Information for <code>invoke-training</code> developers.</li> </ul>"},{"location":"contributing/development_environment/","title":"Development Environment Setup","text":"<p>See the developer installation instructions.</p>"},{"location":"contributing/directory_structure/","title":"Directory Structure","text":"<pre><code>invoke-training/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 invoke-training/\n\u2502       \u251c\u2500\u2500 _shared/ # Utilities shared across multiple pipelines. Hight unit test coverage.\n\u2502       \u251c\u2500\u2500 config/ # Config structures shared by multiple pipelines.\n\u2502       \u251c\u2500\u2500 pipelines/ # Each pipeline is isolated in it's own directory with a train.py and config.py.\n\u2502       \u2502   \u251c\u2500\u2500 stable_diffusion/\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 lora/\n\u2502       \u2502   \u2502   \u2502   \u251c\u2500\u2500 config.py\n\u2502       \u2502   \u2502   \u2502   \u2514\u2500\u2500 train.py\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 textual_inversion/\n\u2502       \u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502       \u2502   \u251c\u2500\u2500 stable_diffusion_xl/\n\u2502       \u2502   \u2514\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 scripts/ # Main entrypoints.\n\u2514\u2500\u2500 tests/ # Mirrors src/ directory.\n</code></pre>"},{"location":"contributing/documentation/","title":"Documentation","text":"<p>The documentation site is generated using mkdocs and mkdocstrings-python.</p> <p>To view your documentation changes locally, run <code>mkdocs serve</code>.</p>"},{"location":"contributing/tests/","title":"Tests","text":"<p>Run all unit tests with:</p> <pre><code>pytest tests/\n</code></pre> <p>There are some test 'markers' defined in pyproject.toml that can be used to skip some tests. For example, the following command skips tests that require a GPU or require downloading model weights:</p> <pre><code>pytest tests/ -m \"not cuda and not loads_model\"\n</code></pre>"},{"location":"get-started/installation/","title":"Installation","text":""},{"location":"get-started/installation/#requirements","title":"Requirements","text":"<ol> <li>Python 3.10, 3.11 and 3.12 are currently supported. Check your Python version by running <code>python -V</code>.</li> <li>An NVIDIA GPU with &gt;= 8 GB VRAM is recommended for model training.</li> </ol>"},{"location":"get-started/installation/#basic-installation","title":"Basic Installation","text":"<ol> <li>Open your terminal and navigate to the directory where you want to clone the <code>invoke-training</code> repo.</li> <li>Clone the repo:</li> </ol> <pre><code>git clone https://github.com/invoke-ai/invoke-training.git\n</code></pre> <ol> <li>Create and activate a python virtual environment. This creates an isolated environment for <code>invoke-training</code> and its dependencies that won't interfere with other python environments on your system, including any installations of InvokeAI.</li> </ol> <pre><code># Navigate to the invoke-training directory.\ncd invoke-training\n\n# Create a new virtual environment named `invoketraining`.\npython -m venv invoketraining\n\n# Activate the new virtual environment.\n# On Windows:\n.\\invoketraining\\Scripts\\activate\n# On MacOS / Linux:\nsource invoketraining/bin/activate\n</code></pre> <ol> <li>Install <code>invoke-training</code> and its dependencies. Run the appropriate install command for your system.</li> </ol> <pre><code># A recent version of pip is required, so first upgrade pip:\npython -m pip install --upgrade pip\n\n# Install - Windows or Linux with a Nvidia GPU:\npip install \".[test]\" --extra-index-url https://download.pytorch.org/whl/cu126\n\n# Install - Linux with no GPU:\npip install \".[test]\" --extra-index-url https://download.pytorch.org/whl/cpu\n\n# Install - All other systems:\npip install \".[test]\"\n</code></pre> <p>In the future, before you run <code>invoke-training</code>, you must activate the virtual environment you created during installation, using the same command you used during installation.</p>"},{"location":"get-started/installation/#developer-installation","title":"Developer Installation","text":"<p>Consider forking the repo if you plan to contribute code changes.</p> <p>Follow the above installation instructions, cloning your fork instead of this repo if you made a fork.</p> <p>Next, we suggest setting up the repo's pre-commit hooks to automatically format and lint your contributions:</p> <ol> <li>(Optional) Install the pre-commit hooks: <code>pre-commit install</code>. This will run static analysis tools (ruff) on <code>git commit</code>.</li> <li>(Optional) Setup <code>ruff</code> in your IDE of choice.</li> </ol>"},{"location":"get-started/quick-start/","title":"Quick Start","text":"<p><code>invoke-training</code> has both a GUI and a CLI (for advanced users). The instructions for getting started with both options can be found on this page.</p> <p>There is also a video introduction to <code>invoke-training</code>:</p>"},{"location":"get-started/quick-start/#quick-start-gui","title":"Quick Start - GUI","text":""},{"location":"get-started/quick-start/#1-installation","title":"1. Installation","text":"<p>Follow the <code>invoke-training</code> installation instructions.</p>"},{"location":"get-started/quick-start/#2-launch-the-gui","title":"2. Launch the GUI","text":"<p>Activate the virtual environment you created during installation, using the same command you used during installation.</p> <p>You'll need to do this every time you run <code>invoke-training</code>.</p> <pre><code># From the invoke-training directory:\ninvoke-train-ui\n\n# Or, you can optionally override the default host and port:\ninvoke-train-ui --host 0.0.0.0 --port 1234\n</code></pre> <p>Access the GUI in your browser at the URL printed to the console.</p>"},{"location":"get-started/quick-start/#3-configure-the-training-job","title":"3. Configure the training job","text":"<p>Select the desired training pipeline type in the top-level tab.</p> <p>For this tutorial, we don't need to change any of the configuration values. The preset configuration should work well.</p>"},{"location":"get-started/quick-start/#4-generate-the-yaml-configuration","title":"4. Generate the YAML configuration","text":"<p>Click on 'Generate Config' to generate a YAML configuration file. This YAML configuration file could be used to launch the training job from the CLI, if desired.</p>"},{"location":"get-started/quick-start/#5-start-training","title":"5. Start training","text":"<p>Click on the 'Start Training' and check your terminal for progress logs.</p>"},{"location":"get-started/quick-start/#6-monitor-training","title":"6. Monitor training","text":"<p>Monitor the training process with Tensorboard by running <code>tensorboard --logdir output/</code> and visiting localhost:6006 in your browser. Here you can see generated validation images throughout the training process.</p> <p> Validation images in the Tensorboard UI.</p>"},{"location":"get-started/quick-start/#7-invokeai","title":"7. Invokeai","text":"<p>Select a checkpoint based on the quality of the generated images.</p> <p>If you haven't already, setup InvokeAI by following its documentation.</p> <p>Copy your selected LoRA checkpoint into your <code>${INVOKEAI_ROOT}/autoimport/lora</code> directory. For example:</p> <pre><code># Note: You will have to replace the timestamp in the checkpoint path.\ncp output/1691088769.5694647/checkpoint_epoch-00000002.safetensors ${INVOKEAI_ROOT}/autoimport/lora/pokemon_epoch-00000002.safetensors\n</code></pre> <p>You can now use your trained Pokemon LoRA in the InvokeAI UI! \ud83c\udf89</p> <p> Example image generated with the prompt \"A cute yoda pokemon creature.\" and Pokemon LoRA.</p>"},{"location":"get-started/quick-start/#quick-start-cli","title":"Quick Start - CLI","text":""},{"location":"get-started/quick-start/#1-installation_1","title":"1. Installation","text":"<p>Follow the <code>invoke-training</code> installation instructions.</p>"},{"location":"get-started/quick-start/#2-training","title":"2. Training","text":"<p>Activate the virtual environment you created during installation, using the same command you used during installation.</p> <p>You'll need to do this every time you run <code>invoke-training</code>.</p> <p>See the Textual Inversion - SDXL tutorial for instructions on how to train a model via the CLI.</p>"},{"location":"guides/dataset_formats/","title":"Dataset Formats","text":"<p><code>invoke-training</code> supports the following dataset formats:</p> <ul> <li><code>IMAGE_CAPTION_JSONL_DATASET</code>: A local image-caption dataset described by a single <code>.jsonl</code> file.</li> <li><code>IMAGE_CAPTION_DIR_DATASET</code>: A local directory of images with associated <code>.txt</code> caption files.</li> <li><code>IMAGE_DIR_DATASET</code>: A local directory of images (without captions).</li> <li><code>HF_HUB_IMAGE_CAPTION_DATASET</code>: A Hugging Face Hub dataset containing images and captions.</li> </ul> <p>See the documentation for a particular training pipeline to see which dataset formats it supports.</p> <p>The following sections explain each of these formats in more detail.</p>"},{"location":"guides/dataset_formats/#image_caption_jsonl_dataset","title":"<code>IMAGE_CAPTION_JSONL_DATASET</code>","text":"<p>Config documentation: ImageCaptionJsonlDatasetConfig</p> <p>A <code>IMAGE_CAPTION_JSONL_DATASET</code> consists of a single <code>.jsonl</code> file containing image paths and associated captions.</p> <p>Sample directory structure: <pre><code>my_custom_dataset/\n\u251c\u2500\u2500 data.jsonl\n\u2514\u2500\u2500 train/\n    \u251c\u2500\u2500 0001.png\n    \u251c\u2500\u2500 0002.png\n    \u251c\u2500\u2500 0003.png\n    \u2514\u2500\u2500 ...\n</code></pre></p> <p>The contents of <code>data.jsonl</code> would be: <pre><code>{\"file_name\": \"train/0001.png\", \"text\": \"This is a caption describing image 0001.\"}\n{\"file_name\": \"train/0002.png\", \"text\": \"This is a caption describing image 0002.\"}\n{\"file_name\": \"train/0003.png\", \"text\": \"This is a caption describing image 0003.\"}\n</code></pre></p> <p>The image file paths can be either absolute paths, or relative to the <code>.jsonl</code> file.</p> <p>Finally, this dataset can be used with the following pipeline dataset configuration: <pre><code>type: IMAGE_CAPTION_JSONL_DATASET\njsonl_path: /path/to/my_custom_dataset/metadata.jsonl\nimage_column: file_name\ncaption_column: text\n</code></pre></p> <p>A useful characteristic of this dataset format is that a <code>.jsonl</code> file can reference an image file anywhere on the local disk. It is common to maintain multiple <code>.jsonl</code> datasets that reference some of the same images without needing multiple copies of those images on disk.</p>"},{"location":"guides/dataset_formats/#image_caption_dir_dataset","title":"<code>IMAGE_CAPTION_DIR_DATASET</code>","text":"<p>Config documentation: ImageCaptionDirDataset</p> <p>A <code>IMAGE_CAPTION_DIR_DATASET</code> consists of a directory of image files and corresponding <code>.txt</code> caption files of the same name.</p> <p>Sample directory structure: <pre><code>my_custom_dataset/\n\u251c\u2500\u2500 0001.png\n\u251c\u2500\u2500 0001.txt\n\u251c\u2500\u2500 0002.jpg\n\u251c\u2500\u2500 0002.txt\n\u251c\u2500\u2500 0003.png\n\u251c\u2500\u2500 0003.txt\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>Each <code>.txt</code> file should contain a caption on the first line of the file. Here are the sample contents of <code>0001.txt</code>: 0001.txt<pre><code>this is a caption for example 0001\n</code></pre></p> <p>This dataset can be used with the following pipeline dataset configuration: <pre><code>type: IMAGE_CAPTION_DIR_DATASET\ndataset_dir: /path/to/my_custom_dataset\n</code></pre></p>"},{"location":"guides/dataset_formats/#image_dir_dataset","title":"<code>IMAGE_DIR_DATASET</code>","text":"<p>Config documentation: ImageDirDataset</p> <p>A <code>IMAGE_DIR_DATASET</code> consists of a single directory of images (without captions).</p> <p>Sample directory structure: <pre><code>my_custom_dataset/\n\u251c\u2500\u2500 0001.png\n\u251c\u2500\u2500 0002.jpg\n\u251c\u2500\u2500 0003.png\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>This dataset can be used with the following pipeline dataset configuration: <pre><code>type: IMAGE_DIR_DATASET\ndataset_dir: /path/to/my_custom_dataset\n</code></pre></p>"},{"location":"guides/dataset_formats/#hf_hub_image_caption_dataset","title":"<code>HF_HUB_IMAGE_CAPTION_DATASET</code>","text":"<p>Config documentation: HFHubImageCaptionDatasetConfig</p> <p>The <code>HF_HUB_IMAGE_CAPTION_DATASET</code> dataset format can be used to access publicly datasets on the Hugging Face Hub. You can filter for the <code>Text-to-Image</code> task to find relevant datasets that contain both an image column and a caption column. lambdalabs/pokemon-blip-captions is a popular choice if you're not sure where to start.</p>"},{"location":"guides/model_merge/","title":"Model Merging","text":"<p><code>invoke-training</code> provides utility scripts for several common model merging workflows. This page contains a summary of the available tools.</p>"},{"location":"guides/model_merge/#extract_lora_from_model_diffpy","title":"<code>extract_lora_from_model_diff.py</code>","text":"<p>Extract a LoRA model that represents the difference between two base models.</p> <p>Note that the extracted LoRA model is a lossy representation of the difference between the models, so some degradation in quality is expected.</p> <p>For usage docs, run: <pre><code>python src/invoke_training/model_merge/scripts/extract_lora_from_model_diff.py -h\n</code></pre></p>"},{"location":"guides/model_merge/#merge_lora_into_modelpy","title":"<code>merge_lora_into_model.py</code>","text":"<p>Merge a LoRA model into a base model to produce a new base model.</p> <p>For usage docs, run: <pre><code>python src/invoke_training/model_merge/scripts/merge_lora_into_model.py -h\n</code></pre></p>"},{"location":"guides/model_merge/#merge_modelspy","title":"<code>merge_models.py</code>","text":"<p>Merge 2 or more base models to produce a single base model (using either LERP or SLERP). This is a simple merge strategy that merges all model weights in the same way.</p> <p>For usage docs, run: <pre><code>python src/invoke_training/model_merge/scripts/merge_models.py -h\n</code></pre></p>"},{"location":"guides/model_merge/#merge_task_models_to_base_modelpy","title":"<code>merge_task_models_to_base_model.py</code>","text":"<p>Merge 1 or more task-specific base models into a single starting base model (using either TIES or DARE). This merge strategy aims to preserve the task-specific behaviors of the task models while making only small changes to the original base model. This approach enables multiple task models to be merged without excessive interference between them.</p> <p>If you want to merge a task-specific LoRA into a base model using this strategy, first use <code>merge_lora_into_model.py</code> to produce a task-specific base model, then merge that new base model using this strategy.</p> <p>For usage docs, run: <pre><code>python src/invoke_training/model_merge/scripts/merge_task_models_to_base_model.py -h\n</code></pre></p>"},{"location":"guides/stable_diffusion/dpo_lora_sd/","title":"(Experimental) Diffusion DPO - SD","text":"<p>Experimental</p> <p>The Diffusion Direct Preference Optimization training pipeline is still experimental. Support may be dropped at any time.</p> <p>This tutorial walks through some initial experiments around using Diffusion Direct Preference Optimization (DPO) (paper) to train Stable Diffusion LoRA models.</p>"},{"location":"guides/stable_diffusion/dpo_lora_sd/#experiment-1-pickapic_v2-lora-training","title":"Experiment 1: <code>pickapic_v2</code> LoRA Training","text":"<p>The Diffusion-DPO paper does full model fine-tuning on the pickapic_v2 dataset, which consists of roughly 1M AI-generated image pairs with preference annotations. In this experiment, we attempt to fine-tune a Stable Diffusion LoRA model using a small subset of the pickapic_v2 dataset.</p> <p>Run this experiment with the following command: <pre><code>invoke-train -c src/invoke_training/sample_configs/_experimental/sd_dpo_lora_pickapic_1x24gb.yaml\n</code></pre></p> <p>Here is a cherry-picked example of a prompt for which this training process was clearly beneficial. Prompt: \"A galaxy-colored figurine is floating over the sea at sunset, photorealistic\"</p> Before DPO Training After DPO Training (same seed)"},{"location":"guides/stable_diffusion/dpo_lora_sd/#experiment-2-lora-model-refinement","title":"Experiment 2: LoRA Model Refinement","text":"<p>As a second experiment, we attempt the following workflow:</p> <ol> <li>Train a Stable Diffusion LoRA model on a particular style.</li> <li>Generate pairs of images of the character with the trained LoRA model.</li> <li>Annotate the preferred image from each pair.</li> <li>Apply Diffusion-DPO to the preference-annotated pairs to further fine-tune the LoRA model.</li> </ol> <p>Note: The steps listed below are pretty rough. They are included primarily for reference for someone looking to resume this line of work in the future.</p>"},{"location":"guides/stable_diffusion/dpo_lora_sd/#1-train-a-style-lora","title":"1. Train a style LoRA","text":"<pre><code>invoke-train -c src/invoke_training/sample_configs/sd_lora_pokemon_1x8gb.yaml\n</code></pre>"},{"location":"guides/stable_diffusion/dpo_lora_sd/#2-generate-images","title":"2. Generate images","text":"<p>Prepare ~100 relevant prompts that will be used to generate training data with the freshly-trained LoRA model. Add the prompts to a <code>.txt</code> file - one prompt per line.</p> <p>Example prompts: <pre><code>a cute orange pokemon character with pointy ears\na drawing of a purple fish\na cartoon blob with a smile on its face\na drawing of a snail with big eyes\n...\n</code></pre></p> <pre><code># Convert the LoRA checkpoint of interest to Kohya format.\n# You will have to change the path timestamps in this example command.\n# TODO(ryand): This manual conversion shouldn't be necessary.\npython src/invoke_training/scripts/convert_sd_lora_to_kohya_format.py \\\n  --src-ckpt-dir output/sd_lora_pokemon/1704824279.2765746/checkpoint_epoch-00000003/ \\\n  --dst-ckpt-file output/sd_lora_pokemon/1704824279.2765746/checkpoint_epoch-00000003_kohya.safetensors\n\n# Generate 2 pairs of images for each prompt.\ninvoke-generate-images \\\n  -o output/pokemon_pairs \\\n  -m runwayml/stable-diffusion-v1-5 \\\n  -v fp16 \\\n  -l output/sd_lora_pokemon/1704824279.2765746/checkpoint_epoch-00000003_kohya.safetensors \\\n  --sd-version SD \\\n  --prompt-file path/to/prompts.txt \\\n  --set-size 2 \\\n  --num-sets 2 \\\n  --height 512 \\\n  --width 512\n</code></pre>"},{"location":"guides/stable_diffusion/dpo_lora_sd/#3-annotate-the-image-pair-preferences","title":"3. Annotate the image pair preferences","text":"<p>Launch the gradio UI for selecting image pair preferences.</p> <pre><code># Note: rank_images.py accepts a full training pipeline config, but only uses the dataset configuration.\npython src/invoke_training/scripts/_experimental/rank_images.py -c src/invoke_training/sample_configs/_experimental/sd_dpo_lora_refinement_pokemon_1x24gb.yaml\n</code></pre> <p>After completing the pair annotations, click \"Save Metadata\" and move the resultant metadata file to your image data directory (e.g. <code>output/pokemon_pairs/metadata.jsonl</code>).</p>"},{"location":"guides/stable_diffusion/dpo_lora_sd/#4-run-diffusion-dpo","title":"4. Run Diffusion-DPO","text":"<pre><code>invoke-train -c src/invoke_training/sample_configs/_experimental/sd_dpo_lora_refinement_pokemon_1x24gb.yaml\n</code></pre>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/","title":"LoRA with Masks - SDXL","text":"<p>This tutorial explains how to prepare masks for an image dataset and then use that dataset to train an SDXL LoRA model.</p> <p>Masks can be used to weight regions of images in a dataset to control how much they contribute to the training process. In this tutorial we will use masks to train on a small dataset of images of Bruce the Gnome (4 images). With such a small dataset, there is a high risk of overfitting to the background elements from the images. We will use masks to avoid this problem ond focus only on the object of interest.</p>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/#1-dataset-preparation","title":"1 - Dataset Preparation","text":"<p>For this tutorial, we'll use a dataset consisting of 4 images of Bruce the Gnome:</p> <p>This sample dataset is included in the invoke-training repo under sample_data/bruce_the_gnome.</p>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/#2-generate-masks","title":"2 - Generate Masks","text":"<p>Use the <code>generate_masks_for_jsonl_dataset.py</code> script to generate masks for your dataset based on a single prompt. In this case we are using the prompt <code>\"a stuffed gnome\"</code>: <pre><code>python src/invoke_training/scripts/_experimental/masks/generate_masks_for_jsonl_dataset.py \\\n  --in-jsonl sample_data/bruce_the_gnome/data.jsonl \\\n  --out-jsonl sample_data/bruce_the_gnome/data_masks.jsonl \\\n  --prompt \"a stuffed gnome\"\n</code></pre></p> <p>The mask generation script will produce the following outputs:</p> <ul> <li>A directory of generated masks: <code>sample_data/bruce_the_gnome/masks/</code></li> <li>A new <code>.jsonl</code> file that references the mask images: <code>sample_data/bruce_the_gnome/data_masks.jsonl</code></li> </ul>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/#3-review-the-generated-masks","title":"3 - Review the Generated Masks","text":"<p>Review the generated masks to make sure that the target regions were masked. You may need to adjust the prompt and re-generate the masks to achieve the desired result. Alternatively, you can edit the masks manually. The masks are simply single-channel grayscale images (0=background, 255=foreground).</p> <p>Here are some examples of the masks that we just generated:</p>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/#4-configuration","title":"4 - Configuration","text":"<p>Below is the training configuration that we'll use for this tutorial.</p> <p>Raw config file: src/invoke_training/sample_configs/sdxl_lora_masks_gnome_1x24gb.yaml.</p> sdxl_lora_masks_gnome_1x24gb.yaml<pre><code># Training mode: LoRA with masks\n# Base model:    SDXL 1.0\n# Dataset:       Bruce the Gnome\n# GPU:           1 x 24GB\n\ntype: SDXL_LORA\nseed: 1\nbase_output_dir: output/bruce/sdxl_lora_masks\n\noptimizer:\n  optimizer_type: AdamW\n  learning_rate: 7e-5\n\nlr_scheduler: constant_with_warmup\nlr_warmup_steps: 50\n\ndata_loader:\n  type: IMAGE_CAPTION_SD_DATA_LOADER\n  dataset:\n    type: IMAGE_CAPTION_JSONL_DATASET\n    jsonl_path: sample_data/bruce_the_gnome/data_masks.jsonl\n  resolution: 1024\n  aspect_ratio_buckets:\n    target_resolution: 1024\n    start_dim: 512\n    end_dim: 1536\n    divisible_by: 128\n\n# General\nmodel: stabilityai/stable-diffusion-xl-base-1.0\n# vae_model: madebyollin/sdxl-vae-fp16-fix\ngradient_accumulation_steps: 1\nweight_dtype: bfloat16\ngradient_checkpointing: True\ncache_vae_outputs: True\n\nmax_train_steps: 500\nsave_every_n_steps: 50\nvalidate_every_n_steps: 50\n\nuse_masks: True\n\nmax_checkpoints: 5\nvalidation_prompts:\n  - A stuffed gnome at the beach with a pina colada in its hand.\n  - A stuffed gnome reading a book in a cozy library.\ntrain_batch_size: 4\nnum_validation_images_per_prompt: 3\n</code></pre> <p>Full documentation of all of the configuration options is here: LoRA SDXL Config</p> <p>There are few things to note about this training config:</p> <ul> <li>We set <code>use_masks: True</code> in order to use the masks that we generated. This configuration is only compatible with datasets that have mask data.</li> <li>The <code>learning_rate</code>, <code>max_train_steps</code>, <code>save_every_n_steps</code>, and <code>validate_every_n_steps</code> are all lower than typical for an SDXL LoRA training pipeline. The combination of masking with the small dataset size cause training to progress very quickly. These configuration fields were all adjusted accordingly to avoid overfitting.</li> </ul>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/#5-start-training","title":"5 - Start Training","text":"<p>Launch the training run. <pre><code># From inside the invoke-training/ source directory:\ninvoke-train -c src/invoke_training/sample_configs/sdxl_lora_masks_gnome_1x24gb.yaml\n</code></pre></p> <p>Training takes ~30 mins on an NVIDIA RTX 4090.</p>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/#4-monitor","title":"4 - Monitor","text":"<p>In a new terminal, launch Tensorboard to monitor the training run: <pre><code>tensorboard --logdir output/\n</code></pre> Access Tensorboard at localhost:6006 in your browser.</p> <p>Sample images will be logged to Tensorboard so that you can see how the model is evolving.</p> <p>Once training is complete, select the model checkpoint that produces the best visual results. For this tutorial, we'll use the checkpoint from step 300:</p> <p> Screenshot of the Tensorboard UI showing the validation images for epoch 300. The validation prompt was: \"A stuffed gnome at the beach with a pina colada in its hand.\".</p>"},{"location":"guides/stable_diffusion/gnome_lora_masks_sdxl/#6-import-into-invokeai","title":"6 - Import into InvokeAI","text":"<p>If you haven't already, setup InvokeAI by following its documentation.</p> <p>Import your trained LoRA model from the 'Models' tab.</p> <p>Congratulations, you can now use your new Bruce-the-Gnome model! \ud83c\udf89</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/","title":"Finetune - SDXL","text":"<p>This tutorial explains how to do a full finetune training run on a Stable Diffusion XL base model.</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#0-prerequisites","title":"0 - Prerequisites","text":"<p>Full model finetuning is more compute-intensive than parameter-efficient finetuning alternatives (e.g. LoRA or Textual Inversion). This tutorial requires a minimum of 24GB of GPU VRAM.</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#1-dataset-preparation","title":"1 - Dataset Preparation","text":"<p>For this tutorial, we will use a dataset consisting of 14 images of robocats. The images were auto-captioned. Here are some sample images from the dataset, including their captions:</p> A white robot with blue eyes and a yellow nose sits on a rock, gazing at the camera, with a pink tree and a white cat in the background. A white cat with green eyes and a blue collar sits on a moss-covered rock in a forest, gazing directly at the camera."},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#2-configuration","title":"2 - Configuration","text":"<p>Below is the training configuration that we'll use for this tutorial.</p> <p>Raw config file: src/invoke_training/sample_configs/sdxl_finetune_robocats_1x24gb.yaml.</p> sdxl_finetune_robocats_1x24gb.yaml<pre><code># Training mode: Full finetune\n# Base model:    SDXL\n# Dataset:       Robocats\n# GPU:           1 x 24GB\n\ntype: SDXL_FINETUNE\nseed: 1\nbase_output_dir: output/robocats/sdxl_finetune\n\noptimizer:\n  optimizer_type: AdamW\n  learning_rate: 2e-5\n  use_8bit: True\n\nlr_scheduler: constant_with_warmup\nlr_warmup_steps: 200\n\ndata_loader:\n  type: IMAGE_CAPTION_SD_DATA_LOADER\n  dataset:\n    type: IMAGE_CAPTION_JSONL_DATASET\n    # Update the jsonl_path field to point to the metadata.jsonl file of the downloaded dataset.\n    jsonl_path: /home/ryan/data/robocats/data.jsonl\n  resolution: 1024\n  aspect_ratio_buckets:\n    target_resolution: 1024\n    start_dim: 512\n    end_dim: 1536\n    divisible_by: 128\n  caption_prefix: \"In the robocat style,\"\n\n# General\nmodel: stabilityai/stable-diffusion-xl-base-1.0\nsave_checkpoint_format: trained_only_diffusers\n# vae_model: madebyollin/sdxl-vae-fp16-fix\nsave_dtype: float16\ngradient_accumulation_steps: 1\nweight_dtype: bfloat16\ngradient_checkpointing: True\ncache_vae_outputs: True\ncache_text_encoder_outputs: True\n\nmax_train_steps: 2000\nvalidate_every_n_steps: 200\nsave_every_n_steps: 2000\n# We save a max of 1 checkpoint for demo purposes, because the checkpoints take up a lot of disk space.\nmax_checkpoints: 1\n\nvalidation_prompts:\n  - In the robocat style, a robotic lion in the jungle.\n  - In the robocat style, a hamburger and fries.\ntrain_batch_size: 4\nnum_validation_images_per_prompt: 3\n</code></pre> <p>Full documentation of all of the configuration options is here: Finetune SDXL Config</p> <p><code>save_checkpoint_format</code></p> <p>Note the <code>save_checkpoint_format</code> setting, as it is unique to full finetune training. For this tutorial, we have set <code>save_checkpoint_format: trained_only_diffusers</code>. This means that only the UNet model will be saved at each checkpoint, and it will be saved in diffusers format. This setting conserves disk space by not redundantly saving the non-trained weights. Before these UNet checkpoints can be used, they must either be merged into a full model, or extracted into a LoRA. Instructions for this follow later in this tutorial. A full explanation of the <code>save_checkpoint_format</code> options can be found here:  save_checkpoint_format.</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#3-start-training","title":"3 - Start Training","text":"<p>Launch the training run. <pre><code># From inside the invoke-training/ source directory:\ninvoke-train -c src/invoke_training/sample_configs/sdxl_finetune_robocats_1x24gb.yaml\n</code></pre></p> <p>Training takes ~45 mins on an NVIDIA RTX 4090.</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#4-monitor","title":"4 - Monitor","text":"<p>In a new terminal, launch Tensorboard to monitor the training run: <pre><code>tensorboard --logdir output/\n</code></pre> Access Tensorboard at localhost:6006 in your browser.</p> <p>Sample images will be logged to Tensorboard so that you can see how the model is evolving.</p> <p>Once training is complete, select the model checkpoint that produces the best visual results.</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#5-prepare-the-trained-model","title":"5 - Prepare the trained model","text":"<p>Since we set <code>save_checkpoint_format: trained_only_diffusers</code>, our selected checkpoint only contains the UNet model weights. The checkpoint has the following directory structure:</p> <pre><code>output/robocats/sdxl_finetune/1715373799.3558652/checkpoints/checkpoint-epoch_00000500-step_00002000/\n\u2514\u2500\u2500 unet\n    \u251c\u2500\u2500 config.json\n    \u2514\u2500\u2500 diffusion_pytorch_model.safetensors\n</code></pre> <p>Before we can use this trained model, we must do one of the following:</p> <ul> <li>Prepare a full diffusers checkpoint with the new UNet weights.</li> <li>Extract the difference between the trained UNet and the original UNet into a LoRA model.</li> </ul>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#prepare-a-full-model","title":"Prepare a full model","text":"<p>If we want to use our finetuned UNet model, we must first package it into a format supported by applications like InvokeAI.</p> <p>In this section we will assume that we have a full SDXL base model in diffusers format. It should have a directory structure like the one shown before. We simply need to replace the <code>unet/</code> directory with the one from our selected training checkpoint: <pre><code>stable-diffusion-xl-base-1.0\n\u251c\u2500\u2500 model_index.json\n\u251c\u2500\u2500 scheduler\n\u2502   \u2514\u2500\u2500 scheduler_config.json\n\u251c\u2500\u2500 text_encoder\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u2514\u2500\u2500 model.fp16.safetensors\n\u251c\u2500\u2500 text_encoder_2\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u2514\u2500\u2500 model.fp16.safetensors\n\u251c\u2500\u2500 tokenizer\n\u2502   \u251c\u2500\u2500 merges.txt\n\u2502   \u251c\u2500\u2500 special_tokens_map.json\n\u2502   \u251c\u2500\u2500 tokenizer_config.json\n\u2502   \u2514\u2500\u2500 vocab.json\n\u251c\u2500\u2500 tokenizer_2\n\u2502   \u251c\u2500\u2500 merges.txt\n\u2502   \u251c\u2500\u2500 special_tokens_map.json\n\u2502   \u251c\u2500\u2500 tokenizer_config.json\n\u2502   \u2514\u2500\u2500 vocab.json\n\u251c\u2500\u2500 unet # &lt;-- Replace this directory with the trained checkpoint.\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u2514\u2500\u2500 diffusion_pytorch_model.fp16.safetensors\n\u251c\u2500\u2500 vae\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u2514\u2500\u2500 diffusion_pytorch_model.fp16.safetensors\n\u2514\u2500\u2500 vae_1_0\n    \u2514\u2500\u2500 diffusion_pytorch_model.fp16.safetensors\n</code></pre></p> <p>diffusers variants (e.g. 'fp16')</p> <p>In this example, notice that the <code>*.safetensors</code> files contain <code>.fp16.</code> in their filenames. Hugging Face refers to this identifier as a \"variant\". It is used to select between multiple model variants in their model hub. In this case, we should add the <code>.fp16.</code> variant tag to our finetuned UNet for consistency with the rest of the model. Since we set <code>save_dtype: float16</code> in our training config, the <code>fp16</code> tag accurately represents the precision of our UNet model file.</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#extract-a-lora-model","title":"Extract a LoRA model","text":"<p>An alternative to using the finetuned UNet model directly is to compare it against the original and extract the difference as a LoRA model. The resultant LoRA has a much smaller file size and can be applied to any base model. But, the LoRA model is a lossy representation of the difference, so some quality degradation is expected.</p> <p>To extract a LoRA model, run the following command: <pre><code>python src/invoke_training/model_merge/scripts/extract_lora_from_model_diff.py \\\n  --model-type SDXL \\\n  --model-orig path/to/stable-diffusion-xl-base-1.0 \\\n  --model-tuned output/robocats/sdxl_finetune/1715373799.3558652/checkpoints/checkpoint-epoch_00000500-step_00002000 \\\n  --save-to robocats_lora_step_2000.safetensors \\\n  --lora-rank 32\n</code></pre></p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#6-import-into-invokeai","title":"6 - Import into InvokeAI","text":"<p>If you haven't already, setup InvokeAI by following its documentation.</p> <p>Import your finetuned diffusers model or your extracted LoRA from the 'Models' tab.</p> <p>Congratulations, you can now use your new robocat model! \ud83c\udf89</p>"},{"location":"guides/stable_diffusion/robocats_finetune_sdxl/#7-comparison-finetune-vs-lora-extraction","title":"7 - Comparison: Finetune vs. LoRA Extraction","text":"<p>As noted earlier, the LoRA extraction process is lossy for a number of reasons.</p> <p>Below, we compare images generated with the same seed and prompt for 3 different model configurations.</p> <p>Prompt: In robocat style, a robotic lion in the jungle.</p> SDXL Base 1.0 w/ Finetuned UNet w/ Extracted LoRA"},{"location":"guides/stable_diffusion/textual_inversion_sdxl/","title":"Textual Inversion - SDXL","text":"<p>This tutorial walks through a Textual Inversion training run with a Stable Diffusion XL base model.</p>"},{"location":"guides/stable_diffusion/textual_inversion_sdxl/#1-dataset","title":"1 - Dataset","text":"<p>For this tutorial, we'll use a dataset consisting of 4 images of Bruce the Gnome:</p> <p>This sample dataset is included in the invoke-training repo under sample_data/bruce_the_gnome.</p> <p>Here are a few tips for preparing a Textual Inversion dataset:</p> <ul> <li>Aim for 4 to 50 images of your concept (object / style). The optimal number depends on many factors, and can be much higher than this for some use cases.</li> <li>Vary all of the image features that you don't want your TI embedding to contain (e.g. background, pose, lighting, etc.).</li> </ul>"},{"location":"guides/stable_diffusion/textual_inversion_sdxl/#2-configuration","title":"2 - Configuration","text":"<p>Below is the training configuration that we'll use for this tutorial.</p> <p>Raw config file: src/invoke_training/sample_configs/sdxl_textual_inversion_gnome_1x24gb.yaml.</p> <p>Full config reference docs: Textual Inversion SDXL Config</p> sdxl_textual_inversion_gnome_1x24gb.yaml<pre><code># Training mode: Textual Inversion\n# Base model:    SDXL\n# GPU:           1 x 24GB\n\ntype: SDXL_TEXTUAL_INVERSION\nseed: 1\nbase_output_dir: output/bruce/sdxl_ti\n\noptimizer:\n  optimizer_type: AdamW\n  learning_rate: 2e-3\n\nlr_warmup_steps: 200\nlr_scheduler: cosine\n\ndata_loader:\n  type: TEXTUAL_INVERSION_SD_DATA_LOADER\n  dataset:\n    type: IMAGE_DIR_DATASET\n    dataset_dir: \"sample_data/bruce_the_gnome\"\n    keep_in_memory: True\n  caption_preset: object\n  resolution: 1024\n  center_crop: True\n  random_flip: False\n  shuffle_caption_delimiter: null\n  dataloader_num_workers: 4\n\n# General\nmodel: stabilityai/stable-diffusion-xl-base-1.0\nvae_model: madebyollin/sdxl-vae-fp16-fix\nnum_vectors: 4\nplaceholder_token: \"bruce_the_gnome\"\ninitializer_token: \"gnome\"\ncache_vae_outputs: False\ngradient_accumulation_steps: 1\nweight_dtype: bfloat16\ngradient_checkpointing: True\n\nmax_train_steps: 2000\nsave_every_n_steps: 200\nvalidate_every_n_steps: 200\n\nmax_checkpoints: 20\nvalidation_prompts:\n  - A photo of bruce_the_gnome at the beach\n  - A photo of bruce_the_gnome reading a book\ntrain_batch_size: 1\nnum_validation_images_per_prompt: 3\n</code></pre>"},{"location":"guides/stable_diffusion/textual_inversion_sdxl/#3-start-training","title":"3 - Start Training","text":"<p>Install invoke-training, if you haven't already.</p> <p>Launch the Textual Inversion training pipeline: <pre><code># From inside the invoke-training/ source directory:\ninvoke-train -c src/invoke_training/sample_configs/sdxl_textual_inversion_gnome_1x24gb.yaml\n</code></pre></p> <p>Training takes ~40 mins on an NVIDIA RTX 4090.</p>"},{"location":"guides/stable_diffusion/textual_inversion_sdxl/#4-monitor","title":"4 - Monitor","text":"<p>In a new terminal, launch Tensorboard to monitor the training run: <pre><code>tensorboard --logdir output/\n</code></pre> Access Tensorboard at localhost:6006 in your browser.</p> <p>Sample images will be logged to Tensorboard so that you can see how the Textual Inversion embedding is evolving.</p> <p>Once training is complete, select the epoch that produces the best visual results.</p> <p>For this tutorial, we'll choose epoch 500:  Screenshot of the Tensorboard UI showing the validation images for epoch 500.</p>"},{"location":"guides/stable_diffusion/textual_inversion_sdxl/#5-transfer-to-invokeai","title":"5 - Transfer to InvokeAI","text":"<p>If you haven't already, setup InvokeAI by following its documentation.</p> <p>Copy the selected TI embedding into your <code>${INVOKEAI_ROOT}/autoimport/embedding/</code> directory. For example: <pre><code>cp output/sdxl_ti_bruce_the_gnome/1702587511.2273068/checkpoint_epoch-00000500.safetensors ${INVOKEAI_ROOT}/autoimport/embedding/bruce_the_gnome.safetensors\n</code></pre></p> <p>Note that we renamed the file to <code>bruce_the_gnome.safetensors</code>. You can choose any file name, but this will become the token used to reference your embedding. So, in our case, we can refer to our new embedding by including <code>&lt;bruce_the_gnome&gt;</code> in our prompts.</p> <p>Launch Invoke AI and you can now use your new <code>bruce_the_gnome</code> TI embedding! \ud83c\udf89</p> <p> Example image generated with the prompt \"<code>a photo of &lt;bruce_the_gnome&gt; at the park</code>\".</p>"},{"location":"reference/config/","title":"Config Reference","text":"<p>This section contains reference documentation for the <code>invoke-training</code> configuration schema (i.e. documentation for all of the supported training options).</p> <p>This documentation uses python typing semantics to define the configuration schema. Typically the configuration for a training run is specified in a YAML file and then parse against this schema.</p>"},{"location":"reference/config/pipelines/sd_lora/","title":"<code>SdLoraConfig</code>","text":""},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.type","title":"type","text":"<pre><code>type: Literal['SD_LORA'] = 'SD_LORA'\n</code></pre>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.model","title":"model","text":"<pre><code>model: str = 'runwayml/stable-diffusion-v1-5'\n</code></pre> <p>Name or path of the base model to train. Can be in diffusers format, or a single stable diffusion checkpoint file. (E.g. 'runwayml/stable-diffusion-v1-5', '/path/to/realisticVisionV51_v51VAE.safetensors', etc. )</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.hf_variant","title":"hf_variant","text":"<pre><code>hf_variant: str | None = 'fp16'\n</code></pre> <p>The Hugging Face Hub model variant to use. Only applies if <code>model</code> is a Hugging Face Hub model name.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.base_embeddings","title":"base_embeddings","text":"<pre><code>base_embeddings: dict[str, str] = {}\n</code></pre> <p>A mapping of embedding tokens to trained embedding file paths. These embeddings will be applied to the base model before training.</p> <p>Example: <pre><code>base_embeddings = {\n    \"bruce_the_gnome\": \"/path/to/bruce_the_gnome.safetensors\",\n}\n</code></pre></p> <p>Consider also adding the embedding tokens to the <code>data_loader.caption_prefix</code> if they are not already present in the dataset captions.</p> <p>Note that the embeddings themselves are not fine-tuned further, but they will impact the LoRA model training if they are referenced in the dataset captions. The list of embeddings provided here should be the same list used at generation time with the resultant LoRA model.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.lora_checkpoint_format","title":"lora_checkpoint_format","text":"<pre><code>lora_checkpoint_format: Literal[\"invoke_peft\", \"kohya\"] = (\n    \"kohya\"\n)\n</code></pre> <p>The format of the LoRA checkpoint to save. Choose between <code>invoke_peft</code> or <code>kohya</code>.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.train_unet","title":"train_unet","text":"<pre><code>train_unet: bool = True\n</code></pre> <p>Whether to add LoRA layers to the UNet model and train it.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.train_text_encoder","title":"train_text_encoder","text":"<pre><code>train_text_encoder: bool = True\n</code></pre> <p>Whether to add LoRA layers to the text encoder and train it.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.optimizer","title":"optimizer","text":"<pre><code>optimizer: AdamOptimizerConfig | ProdigyOptimizerConfig = (\n    AdamOptimizerConfig()\n)\n</code></pre>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.text_encoder_learning_rate","title":"text_encoder_learning_rate","text":"<pre><code>text_encoder_learning_rate: float | None = None\n</code></pre> <p>The learning rate to use for the text encoder model. If set, this overrides the optimizer's default learning rate.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.unet_learning_rate","title":"unet_learning_rate","text":"<pre><code>unet_learning_rate: float | None = None\n</code></pre> <p>The learning rate to use for the UNet model. If set, this overrides the optimizer's default learning rate.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.lr_scheduler","title":"lr_scheduler","text":"<pre><code>lr_scheduler: Literal[\n    \"linear\",\n    \"cosine\",\n    \"cosine_with_restarts\",\n    \"polynomial\",\n    \"constant\",\n    \"constant_with_warmup\",\n] = \"constant\"\n</code></pre>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.lr_warmup_steps","title":"lr_warmup_steps","text":"<pre><code>lr_warmup_steps: int = 0\n</code></pre> <p>The number of warmup steps in the learning rate scheduler. Only applied to schedulers that support warmup. See lr_scheduler.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.min_snr_gamma","title":"min_snr_gamma","text":"<pre><code>min_snr_gamma: float | None = 5.0\n</code></pre> <p>Min-SNR weighting for diffusion training was introduced in https://arxiv.org/abs/2303.09556. This strategy improves the speed of training convergence by adjusting the weight of each sample.</p> <p><code>min_snr_gamma</code> acts like an an upper bound on the weight of samples with low noise levels.</p> <p>If <code>None</code>, then Min-SNR weighting will not be applied. If enabled, the recommended value is <code>min_snr_gamma = 5.0</code>.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.lora_rank_dim","title":"lora_rank_dim","text":"<pre><code>lora_rank_dim: int = 4\n</code></pre> <p>The rank dimension to use for the LoRA layers. Increasing the rank dimension increases the model's expressivity, but also increases the size of the generated LoRA model.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.unet_lora_target_modules","title":"unet_lora_target_modules","text":"<pre><code>unet_lora_target_modules: list[str] = UNET_TARGET_MODULES\n</code></pre> <p>The list of target modules to apply LoRA layers to in the UNet model. The default list will produce a highly expressive LoRA model.</p> <p>For a smaller and less expressive LoRA model, the following list is recommended: <pre><code>unet_lora_target_modules = [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n</code></pre></p> <p>The list of target modules is passed to Hugging Face's PEFT library. See the docs for details.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.text_encoder_lora_target_modules","title":"text_encoder_lora_target_modules","text":"<pre><code>text_encoder_lora_target_modules: list[str] = (\n    TEXT_ENCODER_TARGET_MODULES\n)\n</code></pre> <p>The list of target modules to apply LoRA layers to in the text encoder models. The default list will produce a highly expressive LoRA model.</p> <p>For a smaller and less expressive LoRA model, the following list is recommended: <pre><code>text_encoder_lora_target_modules = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n</code></pre></p> <p>The list of target modules is passed to Hugging Face's PEFT library. See the docs for details.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.cache_text_encoder_outputs","title":"cache_text_encoder_outputs","text":"<pre><code>cache_text_encoder_outputs: bool = False\n</code></pre> <p>If True, the text encoder(s) will be applied to all of the captions in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the text encoders in VRAM), and speeds up training  (don't have to run the text encoders for each training example). This option can only be enabled if <code>train_text_encoder == False</code> and there are no caption augmentations being applied.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.cache_vae_outputs","title":"cache_vae_outputs","text":"<pre><code>cache_vae_outputs: bool = False\n</code></pre> <p>If True, the VAE will be applied to all of the images in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the VAE in VRAM), and speeds up training (don't have to run the VAE encoding step). This option can only be enabled if all non-deterministic image augmentations are disabled (i.e. center_crop=True, random_flip=False).</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.enable_cpu_offload_during_validation","title":"enable_cpu_offload_during_validation","text":"<pre><code>enable_cpu_offload_during_validation: bool = False\n</code></pre> <p>If True, models will be kept in CPU memory and loaded into GPU memory one-by-one while generating validation images. This reduces VRAM requirements at the cost of slower generation of validation images.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<pre><code>gradient_accumulation_steps: int = 1\n</code></pre> <p>The number of gradient steps to accumulate before each weight update. This value is passed to Hugging Face Accelerate. This is an alternative to increasing the batch size when training with limited VRAM.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.weight_dtype","title":"weight_dtype","text":"<pre><code>weight_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = (\n    \"bfloat16\"\n)\n</code></pre> <p>All weights (trainable and fixed) will be cast to this precision. Lower precision dtypes require less VRAM, and result in faster training, but are more prone to issues with numerical stability.</p> <p>Recommendations:</p> <ul> <li><code>\"float32\"</code>: Use this mode if you have plenty of VRAM available.</li> <li><code>\"bfloat16\"</code>: Use this mode if you have limited VRAM and a GPU that supports bfloat16.</li> <li><code>\"float16\"</code>: Use this mode if you have limited VRAM and a GPU that does not support bfloat16.</li> </ul> <p>See also <code>mixed_precision</code>.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.mixed_precision","title":"mixed_precision","text":"<pre><code>mixed_precision: Literal[\"no\", \"fp16\", \"bf16\", \"fp8\"] = \"no\"\n</code></pre> <p>The mixed precision mode to use.</p> <p>If mixed precision is enabled, then all non-trainable parameters will be cast to the specified <code>weight_dtype</code>, and trainable parameters are kept in float32 precision to avoid issues with numerical stability.</p> <p>This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.mixed_precision</code> for more details.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.xformers","title":"xformers","text":"<pre><code>xformers: bool = False\n</code></pre> <p>If true, use xformers for more efficient attention blocks.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.gradient_checkpointing","title":"gradient_checkpointing","text":"<pre><code>gradient_checkpointing: bool = False\n</code></pre> <p>Whether or not to use gradient checkpointing to save memory at the expense of a slower backward pass. Enabling gradient checkpointing slows down training by ~20%.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.max_checkpoints","title":"max_checkpoints","text":"<pre><code>max_checkpoints: int | None = None\n</code></pre> <p>The maximum number of checkpoints to keep. New checkpoints will replace earlier checkpoints to stay under this limit. Note that this limit is applied to 'step' and 'epoch' checkpoints separately.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.prediction_type","title":"prediction_type","text":"<pre><code>prediction_type: (\n    Literal[\"epsilon\", \"v_prediction\"] | None\n) = None\n</code></pre> <p>The prediction_type that will be used for training. Choose between 'epsilon' or 'v_prediction' or leave 'None'. If 'None', the prediction type of the scheduler: <code>noise_scheduler.config.prediction_type</code> is used.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.max_grad_norm","title":"max_grad_norm","text":"<pre><code>max_grad_norm: float | None = None\n</code></pre> <p>Max gradient norm for clipping. Set to None for no clipping.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.validation_prompts","title":"validation_prompts","text":"<pre><code>validation_prompts: list[str] = []\n</code></pre> <p>A list of prompts that will be used to generate images throughout training for the purpose of tracking progress. See also 'validate_every_n_epochs'.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.negative_validation_prompts","title":"negative_validation_prompts","text":"<pre><code>negative_validation_prompts: list[str] | None = None\n</code></pre> <p>A list of negative prompts that will be applied when generating validation images. If set, this list should have the same length as 'validation_prompts'.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.num_validation_images_per_prompt","title":"num_validation_images_per_prompt","text":"<pre><code>num_validation_images_per_prompt: int = 4\n</code></pre> <p>The number of validation images to generate for each prompt in 'validation_prompts'. Careful, validation can become quite slow if this number is too large.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.train_batch_size","title":"train_batch_size","text":"<pre><code>train_batch_size: int = 4\n</code></pre> <p>The training batch size.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.use_masks","title":"use_masks","text":"<pre><code>use_masks: bool = False\n</code></pre> <p>If True, image masks will be applied to weight the loss during training. The dataset must contain masks for this feature to be used.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.data_loader","title":"data_loader","text":"<pre><code>data_loader: Annotated[\n    Union[\n        ImageCaptionSDDataLoaderConfig,\n        DreamboothSDDataLoaderConfig,\n    ],\n    Field(discriminator=type),\n]\n</code></pre>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.seed","title":"seed","text":"<pre><code>seed: Optional[int] = None\n</code></pre> <p>A randomization seed for reproducible training. Set to any constant integer for consistent training results. If set to <code>null</code>, training will be non-deterministic.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.base_output_dir","title":"base_output_dir","text":"<pre><code>base_output_dir: str\n</code></pre> <p>The output directory where the training outputs (model checkpoints, logs, intermediate predictions) will be written. A subdirectory will be created with a timestamp for each new training run.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.report_to","title":"report_to","text":"<pre><code>report_to: Literal[\n    \"all\", \"tensorboard\", \"wandb\", \"comet_ml\"\n] = \"tensorboard\"\n</code></pre> <p>The integration to report results and logs to. This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.log_with</code> for more details.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.max_train_steps","title":"max_train_steps","text":"<pre><code>max_train_steps: int | None = None\n</code></pre> <p>Total number of training steps to perform. One training step is one gradient update.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.max_train_epochs","title":"max_train_epochs","text":"<pre><code>max_train_epochs: int | None = None\n</code></pre> <p>Total number of training epochs to perform. One epoch is one pass over the entire dataset.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.save_every_n_epochs","title":"save_every_n_epochs","text":"<pre><code>save_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.save_every_n_steps","title":"save_every_n_steps","text":"<pre><code>save_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.validate_every_n_epochs","title":"validate_every_n_epochs","text":"<pre><code>validate_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.validate_every_n_steps","title":"validate_every_n_steps","text":"<pre><code>validate_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_lora/#invoke_training.pipelines.stable_diffusion.lora.config.SdLoraConfig.check_validation_prompts","title":"check_validation_prompts","text":"<pre><code>check_validation_prompts()\n</code></pre>"},{"location":"reference/config/pipelines/sd_textual_inversion/","title":"<code>SdTextualInversionConfig</code>","text":""},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.type","title":"type","text":"<pre><code>type: Literal[\"SD_TEXTUAL_INVERSION\"] = (\n    \"SD_TEXTUAL_INVERSION\"\n)\n</code></pre> <p>Must be <code>SD_TEXTUAL_INVERSION</code>. This is what differentiates training pipeline types.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.model","title":"model","text":"<pre><code>model: str\n</code></pre> <p>Name or path of the base model to train. Can be in diffusers format, or a single stable diffusion checkpoint file. (E.g. <code>\"runwayml/stable-diffusion-v1-5\"</code>, <code>\"stabilityai/stable-diffusion-xl-base-1.0\"</code>, <code>\"/path/to/local/model.safetensors\"</code>, etc.)</p> <p>The model architecture must match the training pipeline being run. For example, if running a Textual Inversion SDXL pipeline, then <code>model</code> must refer to an SDXL model.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.hf_variant","title":"hf_variant","text":"<pre><code>hf_variant: str | None = 'fp16'\n</code></pre> <p>The Hugging Face Hub model variant to use. Only applies if <code>model</code> is a Hugging Face Hub model name.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.num_vectors","title":"num_vectors","text":"<pre><code>num_vectors: int = 1\n</code></pre> <p>Note: <code>num_vectors</code> can be overridden by <code>initial_phrase</code>.</p> <p>The number of textual inversion embedding vectors that will be used to learn the concept.</p> <p>Increasing the <code>num_vectors</code> enables the model to learn more complex concepts, but has the following drawbacks:</p> <ul> <li>greater risk of overfitting</li> <li>increased size of the resulting output file</li> <li>consumes more of the prompt capacity at inference time</li> </ul> <p>Typical values for <code>num_vectors</code> are in the range [1, 16].</p> <p>As a rule of thumb, <code>num_vectors</code> can be increased as the size of the dataset increases (without overfitting).</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.placeholder_token","title":"placeholder_token","text":"<pre><code>placeholder_token: str\n</code></pre> <p>The special word to associate the learned embeddings with. Choose a unique token that is unlikely to already exist in the tokenizer's vocabulary.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.initializer_token","title":"initializer_token","text":"<pre><code>initializer_token: str | None = None\n</code></pre> <p>Note: Exactly one of <code>initializer_token</code>, <code>initial_embedding_file</code>, or <code>initial_phrase</code> should be set.</p> <p>A vocabulary token to use as an initializer for the placeholder token. It should be a single word that roughly describes the object or style that you're trying to train on. Must map to a single tokenizer token.</p> <p>For example, if you are training on a dataset of images of your pet dog, a good choice would be <code>dog</code>.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.initial_embedding_file","title":"initial_embedding_file","text":"<pre><code>initial_embedding_file: str | None = None\n</code></pre> <p>Note: Exactly one of <code>initializer_token</code>, <code>initial_embedding_file</code>, or <code>initial_phrase</code> should be set.</p> <p>Path to an existing TI embedding that will be used to initialize the embedding being trained. The placeholder token in the file must match the <code>placeholder_token</code> field.</p> <p>Either <code>initializer_token</code> or <code>initial_embedding_file</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.initial_phrase","title":"initial_phrase","text":"<pre><code>initial_phrase: str | None = None\n</code></pre> <p>Note: Exactly one of <code>initializer_token</code>, <code>initial_embedding_file</code>, or <code>initial_phrase</code> should be set.</p> <p>A phrase that will be used to initialize the placeholder token embedding. The phrase will be tokenized, and the corresponding embeddings will be used to initialize the placeholder tokens. The number of embedding vectors will be inferred from the length of the tokenized phrase, so keep the phrase short. The consequences of training a large number of embedding vectors are discussed in the <code>num_vectors</code> field documentation.</p> <p>For example, if you are training on a dataset of images of pokemon, you might use <code>pokemon sketch white background</code>.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.optimizer","title":"optimizer","text":"<pre><code>optimizer: AdamOptimizerConfig | ProdigyOptimizerConfig = (\n    AdamOptimizerConfig()\n)\n</code></pre>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.lr_scheduler","title":"lr_scheduler","text":"<pre><code>lr_scheduler: Literal[\n    \"linear\",\n    \"cosine\",\n    \"cosine_with_restarts\",\n    \"polynomial\",\n    \"constant\",\n    \"constant_with_warmup\",\n] = \"constant\"\n</code></pre>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.lr_warmup_steps","title":"lr_warmup_steps","text":"<pre><code>lr_warmup_steps: int = 0\n</code></pre> <p>The number of warmup steps in the learning rate scheduler. Only applied to schedulers that support warmup. See lr_scheduler.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.min_snr_gamma","title":"min_snr_gamma","text":"<pre><code>min_snr_gamma: float | None = 5.0\n</code></pre> <p>Min-SNR weighting for diffusion training was introduced in https://arxiv.org/abs/2303.09556. This strategy improves the speed of training convergence by adjusting the weight of each sample.</p> <p><code>min_snr_gamma</code> acts like an an upper bound on the weight of samples with low noise levels.</p> <p>If <code>None</code>, then Min-SNR weighting will not be applied. If enabled, the recommended value is <code>min_snr_gamma = 5.0</code>.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.cache_vae_outputs","title":"cache_vae_outputs","text":"<pre><code>cache_vae_outputs: bool = False\n</code></pre> <p>If True, the VAE will be applied to all of the images in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the VAE in VRAM), and speeds up training (don't have to run the VAE encoding step).</p> <p>This option can only be enabled if all non-deterministic image augmentations are disabled (i.e. <code>center_crop=True</code>, <code>random_flip=False</code>, etc.).</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.enable_cpu_offload_during_validation","title":"enable_cpu_offload_during_validation","text":"<pre><code>enable_cpu_offload_during_validation: bool = False\n</code></pre> <p>If True, models will be kept in CPU memory and loaded into GPU memory one-by-one while generating validation images. This reduces VRAM requirements at the cost of slower generation of validation images.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<pre><code>gradient_accumulation_steps: int = 1\n</code></pre> <p>The number of gradient steps to accumulate before each weight update. This is an alternative to increasing the <code>train_batch_size</code> when training with limited VRAM.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.weight_dtype","title":"weight_dtype","text":"<pre><code>weight_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = (\n    \"bfloat16\"\n)\n</code></pre> <p>All weights (trainable and fixed) will be cast to this precision. Lower precision dtypes require less VRAM, and result in faster training, but are more prone to issues with numerical stability.</p> <p>Recommendations:</p> <ul> <li><code>\"float32\"</code>: Use this mode if you have plenty of VRAM available.</li> <li><code>\"bfloat16\"</code>: Use this mode if you have limited VRAM and a GPU that supports bfloat16.</li> <li><code>\"float16\"</code>: Use this mode if you have limited VRAM and a GPU that does not support bfloat16.</li> </ul> <p>See also <code>mixed_precision</code>.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.mixed_precision","title":"mixed_precision","text":"<pre><code>mixed_precision: Literal[\"no\", \"fp16\", \"bf16\", \"fp8\"] = \"no\"\n</code></pre> <p>The mixed precision mode to use.</p> <p>If mixed precision is enabled, then all non-trainable parameters will be cast to the specified <code>weight_dtype</code>, and trainable parameters are kept in float32 precision to avoid issues with numerical stability.</p> <p>This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.mixed_precision</code> for more details.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.xformers","title":"xformers","text":"<pre><code>xformers: bool = False\n</code></pre> <p>If <code>True</code>, use xformers for more efficient attention blocks.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.gradient_checkpointing","title":"gradient_checkpointing","text":"<pre><code>gradient_checkpointing: bool = False\n</code></pre> <p>Whether or not to use gradient checkpointing to save memory at the expense of a slower backward pass. Enabling gradient checkpointing slows down training by ~20%.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.max_checkpoints","title":"max_checkpoints","text":"<pre><code>max_checkpoints: int | None = None\n</code></pre> <p>The maximum number of checkpoints to keep. New checkpoints will replace earlier checkpoints to stay under this limit. Note that this limit is applied to 'step' and 'epoch' checkpoints separately.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.prediction_type","title":"prediction_type","text":"<pre><code>prediction_type: (\n    Literal[\"epsilon\", \"v_prediction\"] | None\n) = None\n</code></pre> <p>The prediction type that will be used for training. If <code>None</code>, the prediction type will be inferred from the scheduler.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.max_grad_norm","title":"max_grad_norm","text":"<pre><code>max_grad_norm: float | None = None\n</code></pre> <p>Maximum gradient norm for gradient clipping. Set to <code>None</code> for no clipping.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.validation_prompts","title":"validation_prompts","text":"<pre><code>validation_prompts: list[str] = []\n</code></pre> <p>A list of prompts that will be used to generate images throughout training for the purpose of tracking progress.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.negative_validation_prompts","title":"negative_validation_prompts","text":"<pre><code>negative_validation_prompts: list[str] | None = None\n</code></pre> <p>A list of negative prompts that will be applied when generating validation images. If set, this list should have the same length as 'validation_prompts'.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.num_validation_images_per_prompt","title":"num_validation_images_per_prompt","text":"<pre><code>num_validation_images_per_prompt: int = 4\n</code></pre> <p>The number of validation images to generate for each prompt in <code>validation_prompts</code>. Careful, validation can become very slow if this number is too large.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.train_batch_size","title":"train_batch_size","text":"<pre><code>train_batch_size: int = 4\n</code></pre> <p>The training batch size.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.use_masks","title":"use_masks","text":"<pre><code>use_masks: bool = False\n</code></pre> <p>If True, image masks will be applied to weight the loss during training. The dataset must contain masks for this feature to be used.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.data_loader","title":"data_loader","text":"<pre><code>data_loader: TextualInversionSDDataLoaderConfig\n</code></pre> <p>The data configuration.</p> <p>See <code>TextualInversionSDDataLoaderConfig</code> for details.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.seed","title":"seed","text":"<pre><code>seed: Optional[int] = None\n</code></pre> <p>A randomization seed for reproducible training. Set to any constant integer for consistent training results. If set to <code>null</code>, training will be non-deterministic.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.base_output_dir","title":"base_output_dir","text":"<pre><code>base_output_dir: str\n</code></pre> <p>The output directory where the training outputs (model checkpoints, logs, intermediate predictions) will be written. A subdirectory will be created with a timestamp for each new training run.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.report_to","title":"report_to","text":"<pre><code>report_to: Literal[\n    \"all\", \"tensorboard\", \"wandb\", \"comet_ml\"\n] = \"tensorboard\"\n</code></pre> <p>The integration to report results and logs to. This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.log_with</code> for more details.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.max_train_steps","title":"max_train_steps","text":"<pre><code>max_train_steps: int | None = None\n</code></pre> <p>Total number of training steps to perform. One training step is one gradient update.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.max_train_epochs","title":"max_train_epochs","text":"<pre><code>max_train_epochs: int | None = None\n</code></pre> <p>Total number of training epochs to perform. One epoch is one pass over the entire dataset.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.save_every_n_epochs","title":"save_every_n_epochs","text":"<pre><code>save_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.save_every_n_steps","title":"save_every_n_steps","text":"<pre><code>save_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.validate_every_n_epochs","title":"validate_every_n_epochs","text":"<pre><code>validate_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.validate_every_n_steps","title":"validate_every_n_steps","text":"<pre><code>validate_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sd_textual_inversion/#invoke_training.pipelines.stable_diffusion.textual_inversion.config.SdTextualInversionConfig.check_validation_prompts","title":"check_validation_prompts","text":"<pre><code>check_validation_prompts()\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_finetune/","title":"<code>SdxlFinetuneConfig</code>","text":""},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.type","title":"type","text":"<pre><code>type: Literal['SDXL_FINETUNE'] = 'SDXL_FINETUNE'\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.model","title":"model","text":"<pre><code>model: str = 'stabilityai/stable-diffusion-xl-base-1.0'\n</code></pre> <p>Name or path of the base model to train. Can be in diffusers format, or a single stable diffusion checkpoint file. (E.g. 'stabilityai/stable-diffusion-xl-base-1.0', '/path/to/JuggernautXL.safetensors', etc. )</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.hf_variant","title":"hf_variant","text":"<pre><code>hf_variant: str | None = 'fp16'\n</code></pre> <p>The Hugging Face Hub model variant to use. Only applies if <code>model</code> is a Hugging Face Hub model name.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.save_checkpoint_format","title":"save_checkpoint_format","text":"<pre><code>save_checkpoint_format: Literal[\n    \"full_diffusers\", \"trained_only_diffusers\"\n] = \"trained_only_diffusers\"\n</code></pre> <p>The save format for the checkpoints.</p> <p>Options:</p> <ul> <li><code>full_diffusers</code>: Save the full model in diffusers format (including models that weren't finetuned). If you want a single output artifact that can be used for generation, then this is the recommended option.</li> <li><code>trained_only_diffusers</code>: Save only the models that were finetuned in diffusers format. For example, if only the UNet model was trained, then only the UNet model will be saved. This option will significantly reduce the disk space consumed by the saved checkpoints. If you plan to extract a LoRA from the fine-tuned model, then this is the recommended option.</li> </ul>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.save_dtype","title":"save_dtype","text":"<pre><code>save_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = (\n    \"float16\"\n)\n</code></pre> <p>The dtype to use when saving the model.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.optimizer","title":"optimizer","text":"<pre><code>optimizer: AdamOptimizerConfig | ProdigyOptimizerConfig = (\n    AdamOptimizerConfig()\n)\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.lr_scheduler","title":"lr_scheduler","text":"<pre><code>lr_scheduler: Literal[\n    \"linear\",\n    \"cosine\",\n    \"cosine_with_restarts\",\n    \"polynomial\",\n    \"constant\",\n    \"constant_with_warmup\",\n] = \"constant\"\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.lr_warmup_steps","title":"lr_warmup_steps","text":"<pre><code>lr_warmup_steps: int = 0\n</code></pre> <p>The number of warmup steps in the learning rate scheduler. Only applied to schedulers that support warmup. See lr_scheduler.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.min_snr_gamma","title":"min_snr_gamma","text":"<pre><code>min_snr_gamma: float | None = 5.0\n</code></pre> <p>Min-SNR weighting for diffusion training was introduced in https://arxiv.org/abs/2303.09556. This strategy improves the speed of training convergence by adjusting the weight of each sample.</p> <p><code>min_snr_gamma</code> acts like an an upper bound on the weight of samples with low noise levels.</p> <p>If <code>None</code>, then Min-SNR weighting will not be applied. If enabled, the recommended value is <code>min_snr_gamma = 5.0</code>.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.cache_text_encoder_outputs","title":"cache_text_encoder_outputs","text":"<pre><code>cache_text_encoder_outputs: bool = False\n</code></pre> <p>If True, the text encoder(s) will be applied to all of the captions in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the text encoders in VRAM), and speeds up training  (don't have to run the text encoders for each training example). This option can only be enabled if <code>train_text_encoder == False</code> and there are no caption augmentations being applied.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.cache_vae_outputs","title":"cache_vae_outputs","text":"<pre><code>cache_vae_outputs: bool = False\n</code></pre> <p>If True, the VAE will be applied to all of the images in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the VAE in VRAM), and speeds up training (don't have to run the VAE encoding step). This option can only be enabled if all non-deterministic image augmentations are disabled (i.e. center_crop=True, random_flip=False).</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.enable_cpu_offload_during_validation","title":"enable_cpu_offload_during_validation","text":"<pre><code>enable_cpu_offload_during_validation: bool = False\n</code></pre> <p>If True, models will be kept in CPU memory and loaded into GPU memory one-by-one while generating validation images. This reduces VRAM requirements at the cost of slower generation of validation images.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<pre><code>gradient_accumulation_steps: int = 1\n</code></pre> <p>The number of gradient steps to accumulate before each weight update. This value is passed to Hugging Face Accelerate. This is an alternative to increasing the batch size when training with limited VRAM.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.weight_dtype","title":"weight_dtype","text":"<pre><code>weight_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = (\n    \"bfloat16\"\n)\n</code></pre> <p>All weights (trainable and fixed) will be cast to this precision. Lower precision dtypes require less VRAM, and result in faster training, but are more prone to issues with numerical stability.</p> <p>Recommendations:</p> <ul> <li><code>\"float32\"</code>: Use this mode if you have plenty of VRAM available.</li> <li><code>\"bfloat16\"</code>: Use this mode if you have limited VRAM and a GPU that supports bfloat16.</li> <li><code>\"float16\"</code>: Use this mode if you have limited VRAM and a GPU that does not support bfloat16.</li> </ul> <p>See also <code>mixed_precision</code>.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.mixed_precision","title":"mixed_precision","text":"<pre><code>mixed_precision: Literal[\"no\", \"fp16\", \"bf16\", \"fp8\"] = \"no\"\n</code></pre> <p>The mixed precision mode to use.</p> <p>If mixed precision is enabled, then all non-trainable parameters will be cast to the specified <code>weight_dtype</code>, and trainable parameters are kept in float32 precision to avoid issues with numerical stability.</p> <p>This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.mixed_precision</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.xformers","title":"xformers","text":"<pre><code>xformers: bool = False\n</code></pre> <p>If true, use xformers for more efficient attention blocks.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.gradient_checkpointing","title":"gradient_checkpointing","text":"<pre><code>gradient_checkpointing: bool = False\n</code></pre> <p>Whether or not to use gradient checkpointing to save memory at the expense of a slower backward pass. Enabling gradient checkpointing slows down training by ~20%.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.max_checkpoints","title":"max_checkpoints","text":"<pre><code>max_checkpoints: int | None = None\n</code></pre> <p>The maximum number of checkpoints to keep. New checkpoints will replace earlier checkpoints to stay under this limit. Note that this limit is applied to 'step' and 'epoch' checkpoints separately.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.prediction_type","title":"prediction_type","text":"<pre><code>prediction_type: (\n    Literal[\"epsilon\", \"v_prediction\"] | None\n) = None\n</code></pre> <p>The prediction_type that will be used for training. Choose between 'epsilon' or 'v_prediction' or leave 'None'. If 'None', the prediction type of the scheduler: <code>noise_scheduler.config.prediction_type</code> is used.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.max_grad_norm","title":"max_grad_norm","text":"<pre><code>max_grad_norm: float | None = None\n</code></pre> <p>Max gradient norm for clipping. Set to None for no clipping.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.validation_prompts","title":"validation_prompts","text":"<pre><code>validation_prompts: list[str] = []\n</code></pre> <p>A list of prompts that will be used to generate images throughout training for the purpose of tracking progress. See also 'validate_every_n_epochs'.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.negative_validation_prompts","title":"negative_validation_prompts","text":"<pre><code>negative_validation_prompts: list[str] | None = None\n</code></pre> <p>A list of negative prompts that will be applied when generating validation images. If set, this list should have the same length as 'validation_prompts'.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.num_validation_images_per_prompt","title":"num_validation_images_per_prompt","text":"<pre><code>num_validation_images_per_prompt: int = 4\n</code></pre> <p>The number of validation images to generate for each prompt in 'validation_prompts'. Careful, validation can become quite slow if this number is too large.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.train_batch_size","title":"train_batch_size","text":"<pre><code>train_batch_size: int = 4\n</code></pre> <p>The training batch size.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.use_masks","title":"use_masks","text":"<pre><code>use_masks: bool = False\n</code></pre> <p>If True, image masks will be applied to weight the loss during training. The dataset must contain masks for this feature to be used.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.data_loader","title":"data_loader","text":"<pre><code>data_loader: Annotated[\n    Union[\n        ImageCaptionSDDataLoaderConfig,\n        DreamboothSDDataLoaderConfig,\n    ],\n    Field(discriminator=type),\n]\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.vae_model","title":"vae_model","text":"<pre><code>vae_model: str | None = None\n</code></pre> <p>The name of the Hugging Face Hub VAE model to train against. This will override the VAE bundled with the base model (specified by the <code>model</code> parameter). This config option is provided for SDXL models, because SDXL shipped with a VAE that produces NaNs in fp16 mode, so it is common to replace this VAE with a fixed version.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.seed","title":"seed","text":"<pre><code>seed: Optional[int] = None\n</code></pre> <p>A randomization seed for reproducible training. Set to any constant integer for consistent training results. If set to <code>null</code>, training will be non-deterministic.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.base_output_dir","title":"base_output_dir","text":"<pre><code>base_output_dir: str\n</code></pre> <p>The output directory where the training outputs (model checkpoints, logs, intermediate predictions) will be written. A subdirectory will be created with a timestamp for each new training run.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.report_to","title":"report_to","text":"<pre><code>report_to: Literal[\n    \"all\", \"tensorboard\", \"wandb\", \"comet_ml\"\n] = \"tensorboard\"\n</code></pre> <p>The integration to report results and logs to. This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.log_with</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.max_train_steps","title":"max_train_steps","text":"<pre><code>max_train_steps: int | None = None\n</code></pre> <p>Total number of training steps to perform. One training step is one gradient update.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.max_train_epochs","title":"max_train_epochs","text":"<pre><code>max_train_epochs: int | None = None\n</code></pre> <p>Total number of training epochs to perform. One epoch is one pass over the entire dataset.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.save_every_n_epochs","title":"save_every_n_epochs","text":"<pre><code>save_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.save_every_n_steps","title":"save_every_n_steps","text":"<pre><code>save_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.validate_every_n_epochs","title":"validate_every_n_epochs","text":"<pre><code>validate_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.validate_every_n_steps","title":"validate_every_n_steps","text":"<pre><code>validate_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_finetune/#invoke_training.pipelines.stable_diffusion_xl.finetune.config.SdxlFinetuneConfig.check_validation_prompts","title":"check_validation_prompts","text":"<pre><code>check_validation_prompts()\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora/","title":"<code>SdxlLoraConfig</code>","text":""},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.type","title":"type","text":"<pre><code>type: Literal['SDXL_LORA'] = 'SDXL_LORA'\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.model","title":"model","text":"<pre><code>model: str = 'stabilityai/stable-diffusion-xl-base-1.0'\n</code></pre> <p>Name or path of the base model to train. Can be in diffusers format, or a single stable diffusion checkpoint file. (E.g. 'stabilityai/stable-diffusion-xl-base-1.0', '/path/to/JuggernautXL.safetensors', etc. )</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.hf_variant","title":"hf_variant","text":"<pre><code>hf_variant: str | None = 'fp16'\n</code></pre> <p>The Hugging Face Hub model variant to use. Only applies if <code>model</code> is a Hugging Face Hub model name.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.base_embeddings","title":"base_embeddings","text":"<pre><code>base_embeddings: dict[str, str] = {}\n</code></pre> <p>A mapping of embedding tokens to trained embedding file paths. These embeddings will be applied to the base model before training.</p> <p>Example: <pre><code>base_embeddings = {\n    \"bruce_the_gnome\": \"/path/to/bruce_the_gnome.safetensors\",\n}\n</code></pre></p> <p>Consider also adding the embedding tokens to the <code>data_loader.caption_prefix</code> if they are not already present in the dataset captions.</p> <p>Note that the embeddings themselves are not fine-tuned further, but they will impact the LoRA model training if they are referenced in the dataset captions. The list of embeddings provided here should be the same list used at generation time with the resultant LoRA model.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.lora_checkpoint_format","title":"lora_checkpoint_format","text":"<pre><code>lora_checkpoint_format: Literal[\"invoke_peft\", \"kohya\"] = (\n    \"kohya\"\n)\n</code></pre> <p>The format of the LoRA checkpoint to save. Choose between <code>invoke_peft</code> or <code>kohya</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.train_unet","title":"train_unet","text":"<pre><code>train_unet: bool = True\n</code></pre> <p>Whether to add LoRA layers to the UNet model and train it.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.train_text_encoder","title":"train_text_encoder","text":"<pre><code>train_text_encoder: bool = True\n</code></pre> <p>Whether to add LoRA layers to the text encoder and train it.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.optimizer","title":"optimizer","text":"<pre><code>optimizer: AdamOptimizerConfig | ProdigyOptimizerConfig = (\n    AdamOptimizerConfig()\n)\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.text_encoder_learning_rate","title":"text_encoder_learning_rate","text":"<pre><code>text_encoder_learning_rate: float | None = None\n</code></pre> <p>The learning rate to use for the text encoder model. If set, this overrides the optimizer's default learning rate.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.unet_learning_rate","title":"unet_learning_rate","text":"<pre><code>unet_learning_rate: float | None = None\n</code></pre> <p>The learning rate to use for the UNet model. If set, this overrides the optimizer's default learning rate.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.lr_scheduler","title":"lr_scheduler","text":"<pre><code>lr_scheduler: Literal[\n    \"linear\",\n    \"cosine\",\n    \"cosine_with_restarts\",\n    \"polynomial\",\n    \"constant\",\n    \"constant_with_warmup\",\n] = \"constant\"\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.lr_warmup_steps","title":"lr_warmup_steps","text":"<pre><code>lr_warmup_steps: int = 0\n</code></pre> <p>The number of warmup steps in the learning rate scheduler. Only applied to schedulers that support warmup. See lr_scheduler.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.min_snr_gamma","title":"min_snr_gamma","text":"<pre><code>min_snr_gamma: float | None = 5.0\n</code></pre> <p>Min-SNR weighting for diffusion training was introduced in https://arxiv.org/abs/2303.09556. This strategy improves the speed of training convergence by adjusting the weight of each sample.</p> <p><code>min_snr_gamma</code> acts like an an upper bound on the weight of samples with low noise levels.</p> <p>If <code>None</code>, then Min-SNR weighting will not be applied. If enabled, the recommended value is <code>min_snr_gamma = 5.0</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.lora_rank_dim","title":"lora_rank_dim","text":"<pre><code>lora_rank_dim: int = 4\n</code></pre> <p>The rank dimension to use for the LoRA layers. Increasing the rank dimension increases the model's expressivity, but also increases the size of the generated LoRA model.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.unet_lora_target_modules","title":"unet_lora_target_modules","text":"<pre><code>unet_lora_target_modules: list[str] = UNET_TARGET_MODULES\n</code></pre> <p>The list of target modules to apply LoRA layers to in the UNet model. The default list will produce a highly expressive LoRA model.</p> <p>For a smaller and less expressive LoRA model, the following list is recommended: <pre><code>unet_lora_target_modules = [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n</code></pre></p> <p>The list of target modules is passed to Hugging Face's PEFT library. See the docs for details.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.text_encoder_lora_target_modules","title":"text_encoder_lora_target_modules","text":"<pre><code>text_encoder_lora_target_modules: list[str] = (\n    TEXT_ENCODER_TARGET_MODULES\n)\n</code></pre> <p>The list of target modules to apply LoRA layers to in the text encoder models. The default list will produce a highly expressive LoRA model.</p> <p>For a smaller and less expressive LoRA model, the following list is recommended: <pre><code>text_encoder_lora_target_modules = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n</code></pre></p> <p>The list of target modules is passed to Hugging Face's PEFT library. See the docs for details.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.cache_text_encoder_outputs","title":"cache_text_encoder_outputs","text":"<pre><code>cache_text_encoder_outputs: bool = False\n</code></pre> <p>If True, the text encoder(s) will be applied to all of the captions in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the text encoders in VRAM), and speeds up training  (don't have to run the text encoders for each training example). This option can only be enabled if <code>train_text_encoder == False</code> and there are no caption augmentations being applied.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.cache_vae_outputs","title":"cache_vae_outputs","text":"<pre><code>cache_vae_outputs: bool = False\n</code></pre> <p>If True, the VAE will be applied to all of the images in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the VAE in VRAM), and speeds up training (don't have to run the VAE encoding step). This option can only be enabled if all non-deterministic image augmentations are disabled (i.e. center_crop=True, random_flip=False).</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.enable_cpu_offload_during_validation","title":"enable_cpu_offload_during_validation","text":"<pre><code>enable_cpu_offload_during_validation: bool = False\n</code></pre> <p>If True, models will be kept in CPU memory and loaded into GPU memory one-by-one while generating validation images. This reduces VRAM requirements at the cost of slower generation of validation images.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<pre><code>gradient_accumulation_steps: int = 1\n</code></pre> <p>The number of gradient steps to accumulate before each weight update. This value is passed to Hugging Face Accelerate. This is an alternative to increasing the batch size when training with limited VRAM.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.weight_dtype","title":"weight_dtype","text":"<pre><code>weight_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = (\n    \"bfloat16\"\n)\n</code></pre> <p>All weights (trainable and fixed) will be cast to this precision. Lower precision dtypes require less VRAM, and result in faster training, but are more prone to issues with numerical stability.</p> <p>Recommendations:</p> <ul> <li><code>\"float32\"</code>: Use this mode if you have plenty of VRAM available.</li> <li><code>\"bfloat16\"</code>: Use this mode if you have limited VRAM and a GPU that supports bfloat16.</li> <li><code>\"float16\"</code>: Use this mode if you have limited VRAM and a GPU that does not support bfloat16.</li> </ul> <p>See also <code>mixed_precision</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.mixed_precision","title":"mixed_precision","text":"<pre><code>mixed_precision: Literal[\"no\", \"fp16\", \"bf16\", \"fp8\"] = \"no\"\n</code></pre> <p>The mixed precision mode to use.</p> <p>If mixed precision is enabled, then all non-trainable parameters will be cast to the specified <code>weight_dtype</code>, and trainable parameters are kept in float32 precision to avoid issues with numerical stability.</p> <p>This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.mixed_precision</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.xformers","title":"xformers","text":"<pre><code>xformers: bool = False\n</code></pre> <p>If true, use xformers for more efficient attention blocks.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.gradient_checkpointing","title":"gradient_checkpointing","text":"<pre><code>gradient_checkpointing: bool = False\n</code></pre> <p>Whether or not to use gradient checkpointing to save memory at the expense of a slower backward pass. Enabling gradient checkpointing slows down training by ~20%.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.max_checkpoints","title":"max_checkpoints","text":"<pre><code>max_checkpoints: int | None = None\n</code></pre> <p>The maximum number of checkpoints to keep. New checkpoints will replace earlier checkpoints to stay under this limit. Note that this limit is applied to 'step' and 'epoch' checkpoints separately.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.prediction_type","title":"prediction_type","text":"<pre><code>prediction_type: (\n    Literal[\"epsilon\", \"v_prediction\"] | None\n) = None\n</code></pre> <p>The prediction_type that will be used for training. Choose between 'epsilon' or 'v_prediction' or leave 'None'. If 'None', the prediction type of the scheduler: <code>noise_scheduler.config.prediction_type</code> is used.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.max_grad_norm","title":"max_grad_norm","text":"<pre><code>max_grad_norm: float | None = None\n</code></pre> <p>Max gradient norm for clipping. Set to None for no clipping.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.validation_prompts","title":"validation_prompts","text":"<pre><code>validation_prompts: list[str] = []\n</code></pre> <p>A list of prompts that will be used to generate images throughout training for the purpose of tracking progress. See also 'validate_every_n_epochs'.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.negative_validation_prompts","title":"negative_validation_prompts","text":"<pre><code>negative_validation_prompts: list[str] | None = None\n</code></pre> <p>A list of negative prompts that will be applied when generating validation images. If set, this list should have the same length as 'validation_prompts'.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.num_validation_images_per_prompt","title":"num_validation_images_per_prompt","text":"<pre><code>num_validation_images_per_prompt: int = 4\n</code></pre> <p>The number of validation images to generate for each prompt in 'validation_prompts'. Careful, validation can become quite slow if this number is too large.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.train_batch_size","title":"train_batch_size","text":"<pre><code>train_batch_size: int = 4\n</code></pre> <p>The training batch size.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.use_masks","title":"use_masks","text":"<pre><code>use_masks: bool = False\n</code></pre> <p>If True, image masks will be applied to weight the loss during training. The dataset must contain masks for this feature to be used.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.data_loader","title":"data_loader","text":"<pre><code>data_loader: Annotated[\n    Union[\n        ImageCaptionSDDataLoaderConfig,\n        DreamboothSDDataLoaderConfig,\n    ],\n    Field(discriminator=type),\n]\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.vae_model","title":"vae_model","text":"<pre><code>vae_model: str | None = None\n</code></pre> <p>The name of the Hugging Face Hub VAE model to train against. This will override the VAE bundled with the base model (specified by the <code>model</code> parameter). This config option is provided for SDXL models, because SDXL shipped with a VAE that produces NaNs in fp16 mode, so it is common to replace this VAE with a fixed version.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.seed","title":"seed","text":"<pre><code>seed: Optional[int] = None\n</code></pre> <p>A randomization seed for reproducible training. Set to any constant integer for consistent training results. If set to <code>null</code>, training will be non-deterministic.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.base_output_dir","title":"base_output_dir","text":"<pre><code>base_output_dir: str\n</code></pre> <p>The output directory where the training outputs (model checkpoints, logs, intermediate predictions) will be written. A subdirectory will be created with a timestamp for each new training run.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.report_to","title":"report_to","text":"<pre><code>report_to: Literal[\n    \"all\", \"tensorboard\", \"wandb\", \"comet_ml\"\n] = \"tensorboard\"\n</code></pre> <p>The integration to report results and logs to. This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.log_with</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.max_train_steps","title":"max_train_steps","text":"<pre><code>max_train_steps: int | None = None\n</code></pre> <p>Total number of training steps to perform. One training step is one gradient update.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.max_train_epochs","title":"max_train_epochs","text":"<pre><code>max_train_epochs: int | None = None\n</code></pre> <p>Total number of training epochs to perform. One epoch is one pass over the entire dataset.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.save_every_n_epochs","title":"save_every_n_epochs","text":"<pre><code>save_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.save_every_n_steps","title":"save_every_n_steps","text":"<pre><code>save_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.validate_every_n_epochs","title":"validate_every_n_epochs","text":"<pre><code>validate_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.validate_every_n_steps","title":"validate_every_n_steps","text":"<pre><code>validate_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora/#invoke_training.pipelines.stable_diffusion_xl.lora.config.SdxlLoraConfig.check_validation_prompts","title":"check_validation_prompts","text":"<pre><code>check_validation_prompts()\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/","title":"<code>SdxlLoraAndTextualInversionConfig</code>","text":""},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.type","title":"type","text":"<pre><code>type: Literal[\"SDXL_LORA_AND_TEXTUAL_INVERSION\"] = (\n    \"SDXL_LORA_AND_TEXTUAL_INVERSION\"\n)\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.model","title":"model","text":"<pre><code>model: str = 'stabilityai/stable-diffusion-xl-base-1.0'\n</code></pre> <p>Name or path of the base model to train. Can be in diffusers format, or a single stable diffusion checkpoint file. (E.g. 'stabilityai/stable-diffusion-xl-base-1.0', '/path/to/JuggernautXL.safetensors', etc. )</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.hf_variant","title":"hf_variant","text":"<pre><code>hf_variant: str | None = 'fp16'\n</code></pre> <p>The Hugging Face Hub model variant to use. Only applies if <code>model</code> is a Hugging Face Hub model name.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.lora_checkpoint_format","title":"lora_checkpoint_format","text":"<pre><code>lora_checkpoint_format: Literal[\"invoke_peft\", \"kohya\"] = (\n    \"kohya\"\n)\n</code></pre> <p>The format of the LoRA checkpoint to save. Choose between <code>invoke_peft</code> or <code>kohya</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.num_vectors","title":"num_vectors","text":"<pre><code>num_vectors: int = 1\n</code></pre> <p>Note: <code>num_vectors</code> can be overridden by <code>initial_phrase</code>.</p> <p>The number of textual inversion embedding vectors that will be used to learn the concept.</p> <p>Increasing the <code>num_vectors</code> enables the model to learn more complex concepts, but has the following drawbacks:</p> <ul> <li>greater risk of overfitting</li> <li>increased size of the resulting output file</li> <li>consumes more of the prompt capacity at inference time</li> </ul> <p>Typical values for <code>num_vectors</code> are in the range [1, 16].</p> <p>As a rule of thumb, <code>num_vectors</code> can be increased as the size of the dataset increases (without overfitting).</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.placeholder_token","title":"placeholder_token","text":"<pre><code>placeholder_token: str\n</code></pre> <p>The special word to associate the learned embeddings with. Choose a unique token that is unlikely to already exist in the tokenizer's vocabulary.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.initializer_token","title":"initializer_token","text":"<pre><code>initializer_token: str | None = None\n</code></pre> <p>A vocabulary token to use as an initializer for the placeholder token. It should be a single word that roughly describes the object or style that you're trying to train on. Must map to a single tokenizer token.</p> <p>For example, if you are training on a dataset of images of your pet dog, a good choice would be <code>dog</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.initial_phrase","title":"initial_phrase","text":"<pre><code>initial_phrase: str | None = None\n</code></pre> <p>Note: Exactly one of <code>initializer_token</code> or <code>initial_phrase</code> should be set.</p> <p>A phrase that will be used to initialize the placeholder token embedding. The phrase will be tokenized, and the corresponding embeddings will be used to initialize the placeholder tokens. The number of embedding vectors will be inferred from the length of the tokenized phrase, so keep the phrase short. The consequences of training a large number of embedding vectors are discussed in the <code>num_vectors</code> field documentation.</p> <p>For example, if you are training on a dataset of images of pokemon, you might use <code>pokemon sketch white background</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.train_unet","title":"train_unet","text":"<pre><code>train_unet: bool = True\n</code></pre> <p>Whether to add LoRA layers to the UNet model and train it.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.train_text_encoder","title":"train_text_encoder","text":"<pre><code>train_text_encoder: bool = True\n</code></pre> <p>Whether to add LoRA layers to the text encoder and train it.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.train_ti","title":"train_ti","text":"<pre><code>train_ti: bool = True\n</code></pre> <p>Whether to train the textual inversion embeddings.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.ti_train_steps_ratio","title":"ti_train_steps_ratio","text":"<pre><code>ti_train_steps_ratio: float | None = None\n</code></pre> <p>The fraction of the total training steps for which the TI embeddings will be trained. For example, if we are training for a total of 5000 steps and <code>ti_train_steps_ratio=0.5</code>, then the TI embeddings will be trained for 2500 steps and the will be frozen for the remaining steps.</p> <p>If <code>None</code>, then the TI embeddings will be trained for the entire duration of training.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.optimizer","title":"optimizer","text":"<pre><code>optimizer: AdamOptimizerConfig | ProdigyOptimizerConfig = (\n    AdamOptimizerConfig()\n)\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.text_encoder_learning_rate","title":"text_encoder_learning_rate","text":"<pre><code>text_encoder_learning_rate: float = 1e-05\n</code></pre> <p>The learning rate to use for the text encoder model.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.unet_learning_rate","title":"unet_learning_rate","text":"<pre><code>unet_learning_rate: float = 0.0001\n</code></pre> <p>The learning rate to use for the UNet model.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.textual_inversion_learning_rate","title":"textual_inversion_learning_rate","text":"<pre><code>textual_inversion_learning_rate: float = 0.001\n</code></pre> <p>The learning rate to use for textual inversion training of the embeddings.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.lr_scheduler","title":"lr_scheduler","text":"<pre><code>lr_scheduler: Literal[\n    \"linear\",\n    \"cosine\",\n    \"cosine_with_restarts\",\n    \"polynomial\",\n    \"constant\",\n    \"constant_with_warmup\",\n] = \"constant\"\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.lr_warmup_steps","title":"lr_warmup_steps","text":"<pre><code>lr_warmup_steps: int = 0\n</code></pre> <p>The number of warmup steps in the learning rate scheduler. Only applied to schedulers that support warmup. See lr_scheduler.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.min_snr_gamma","title":"min_snr_gamma","text":"<pre><code>min_snr_gamma: float | None = 5.0\n</code></pre> <p>Min-SNR weighting for diffusion training was introduced in https://arxiv.org/abs/2303.09556. This strategy improves the speed of training convergence by adjusting the weight of each sample.</p> <p><code>min_snr_gamma</code> acts like an an upper bound on the weight of samples with low noise levels.</p> <p>If <code>None</code>, then Min-SNR weighting will not be applied. If enabled, the recommended value is <code>min_snr_gamma = 5.0</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.lora_rank_dim","title":"lora_rank_dim","text":"<pre><code>lora_rank_dim: int = 4\n</code></pre> <p>The rank dimension to use for the LoRA layers. Increasing the rank dimension increases the model's expressivity, but also increases the size of the generated LoRA model.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.cache_text_encoder_outputs","title":"cache_text_encoder_outputs","text":"<pre><code>cache_text_encoder_outputs: bool = False\n</code></pre> <p>If True, the text encoder(s) will be applied to all of the captions in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the text encoders in VRAM), and speeds up training  (don't have to run the text encoders for each training example). This option can only be enabled if <code>train_text_encoder == False</code> and there are no caption augmentations being applied.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.cache_vae_outputs","title":"cache_vae_outputs","text":"<pre><code>cache_vae_outputs: bool = False\n</code></pre> <p>If True, the VAE will be applied to all of the images in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the VAE in VRAM), and speeds up training (don't have to run the VAE encoding step). This option can only be enabled if all non-deterministic image augmentations are disabled (i.e. center_crop=True, random_flip=False).</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.enable_cpu_offload_during_validation","title":"enable_cpu_offload_during_validation","text":"<pre><code>enable_cpu_offload_during_validation: bool = False\n</code></pre> <p>If True, models will be kept in CPU memory and loaded into GPU memory one-by-one while generating validation images. This reduces VRAM requirements at the cost of slower generation of validation images.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<pre><code>gradient_accumulation_steps: int = 1\n</code></pre> <p>The number of gradient steps to accumulate before each weight update. This value is passed to Hugging Face Accelerate. This is an alternative to increasing the batch size when training with limited VRAM.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.weight_dtype","title":"weight_dtype","text":"<pre><code>weight_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = (\n    \"bfloat16\"\n)\n</code></pre> <p>All weights (trainable and fixed) will be cast to this precision. Lower precision dtypes require less VRAM, and result in faster training, but are more prone to issues with numerical stability.</p> <p>Recommendations:</p> <ul> <li><code>\"float32\"</code>: Use this mode if you have plenty of VRAM available.</li> <li><code>\"bfloat16\"</code>: Use this mode if you have limited VRAM and a GPU that supports bfloat16.</li> <li><code>\"float16\"</code>: Use this mode if you have limited VRAM and a GPU that does not support bfloat16.</li> </ul> <p>See also <code>mixed_precision</code>.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.mixed_precision","title":"mixed_precision","text":"<pre><code>mixed_precision: Literal[\"no\", \"fp16\", \"bf16\", \"fp8\"] = \"no\"\n</code></pre> <p>The mixed precision mode to use.</p> <p>If mixed precision is enabled, then all non-trainable parameters will be cast to the specified <code>weight_dtype</code>, and trainable parameters are kept in float32 precision to avoid issues with numerical stability.</p> <p>This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.mixed_precision</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.xformers","title":"xformers","text":"<pre><code>xformers: bool = False\n</code></pre> <p>If true, use xformers for more efficient attention blocks.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.gradient_checkpointing","title":"gradient_checkpointing","text":"<pre><code>gradient_checkpointing: bool = False\n</code></pre> <p>Whether or not to use gradient checkpointing to save memory at the expense of a slower backward pass. Enabling gradient checkpointing slows down training by ~20%.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.max_checkpoints","title":"max_checkpoints","text":"<pre><code>max_checkpoints: int | None = None\n</code></pre> <p>The maximum number of checkpoints to keep. New checkpoints will replace earlier checkpoints to stay under this limit. Note that this limit is applied to 'step' and 'epoch' checkpoints separately.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.prediction_type","title":"prediction_type","text":"<pre><code>prediction_type: (\n    Literal[\"epsilon\", \"v_prediction\"] | None\n) = None\n</code></pre> <p>The prediction_type that will be used for training. Choose between 'epsilon' or 'v_prediction' or leave 'None'. If 'None', the prediction type of the scheduler: <code>noise_scheduler.config.prediction_type</code> is used.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.max_grad_norm","title":"max_grad_norm","text":"<pre><code>max_grad_norm: float | None = None\n</code></pre> <p>Max gradient norm for clipping. Set to None for no clipping.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.validation_prompts","title":"validation_prompts","text":"<pre><code>validation_prompts: list[str] = []\n</code></pre> <p>A list of prompts that will be used to generate images throughout training for the purpose of tracking progress.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.negative_validation_prompts","title":"negative_validation_prompts","text":"<pre><code>negative_validation_prompts: list[str] | None = None\n</code></pre> <p>A list of negative prompts that will be applied when generating validation images. If set, this list should have the same length as 'validation_prompts'.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.num_validation_images_per_prompt","title":"num_validation_images_per_prompt","text":"<pre><code>num_validation_images_per_prompt: int = 4\n</code></pre> <p>The number of validation images to generate for each prompt in 'validation_prompts'. Careful, validation can become quite slow if this number is too large.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.train_batch_size","title":"train_batch_size","text":"<pre><code>train_batch_size: int = 4\n</code></pre> <p>The training batch size.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.use_masks","title":"use_masks","text":"<pre><code>use_masks: bool = False\n</code></pre> <p>If True, image masks will be applied to weight the loss during training. The dataset must contain masks for this feature to be used.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.data_loader","title":"data_loader","text":"<pre><code>data_loader: TextualInversionSDDataLoaderConfig\n</code></pre> <p>The data configuration.</p> <p>See <code>TextualInversionSDDataLoaderConfig</code> for details.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.vae_model","title":"vae_model","text":"<pre><code>vae_model: str | None = None\n</code></pre> <p>The name of the Hugging Face Hub VAE model to train against. This will override the VAE bundled with the base model (specified by the <code>model</code> parameter). This config option is provided for SDXL models, because SDXL shipped with a VAE that produces NaNs in fp16 mode, so it is common to replace this VAE with a fixed version.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.seed","title":"seed","text":"<pre><code>seed: Optional[int] = None\n</code></pre> <p>A randomization seed for reproducible training. Set to any constant integer for consistent training results. If set to <code>null</code>, training will be non-deterministic.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.base_output_dir","title":"base_output_dir","text":"<pre><code>base_output_dir: str\n</code></pre> <p>The output directory where the training outputs (model checkpoints, logs, intermediate predictions) will be written. A subdirectory will be created with a timestamp for each new training run.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.report_to","title":"report_to","text":"<pre><code>report_to: Literal[\n    \"all\", \"tensorboard\", \"wandb\", \"comet_ml\"\n] = \"tensorboard\"\n</code></pre> <p>The integration to report results and logs to. This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.log_with</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.max_train_steps","title":"max_train_steps","text":"<pre><code>max_train_steps: int | None = None\n</code></pre> <p>Total number of training steps to perform. One training step is one gradient update.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.max_train_epochs","title":"max_train_epochs","text":"<pre><code>max_train_epochs: int | None = None\n</code></pre> <p>Total number of training epochs to perform. One epoch is one pass over the entire dataset.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.save_every_n_epochs","title":"save_every_n_epochs","text":"<pre><code>save_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.save_every_n_steps","title":"save_every_n_steps","text":"<pre><code>save_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.validate_every_n_epochs","title":"validate_every_n_epochs","text":"<pre><code>validate_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.validate_every_n_steps","title":"validate_every_n_steps","text":"<pre><code>validate_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_lora_and_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.lora_and_textual_inversion.config.SdxlLoraAndTextualInversionConfig.check_validation_prompts","title":"check_validation_prompts","text":"<pre><code>check_validation_prompts()\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/","title":"<code>SdxlTextualInversionConfig</code>","text":"<p>Below is a sample yaml config file for Textual Inversion SDXL training (raw file). All of the configuration fields are explained in detail on this page.</p> sdxl_textual_inversion_gnome_1x24gb.yaml<pre><code># Training mode: Textual Inversion\n# Base model:    SDXL\n# GPU:           1 x 24GB\n\ntype: SDXL_TEXTUAL_INVERSION\nseed: 1\nbase_output_dir: output/bruce/sdxl_ti\n\noptimizer:\n  optimizer_type: AdamW\n  learning_rate: 2e-3\n\nlr_warmup_steps: 200\nlr_scheduler: cosine\n\ndata_loader:\n  type: TEXTUAL_INVERSION_SD_DATA_LOADER\n  dataset:\n    type: IMAGE_DIR_DATASET\n    dataset_dir: \"sample_data/bruce_the_gnome\"\n    keep_in_memory: True\n  caption_preset: object\n  resolution: 1024\n  center_crop: True\n  random_flip: False\n  shuffle_caption_delimiter: null\n  dataloader_num_workers: 4\n\n# General\nmodel: stabilityai/stable-diffusion-xl-base-1.0\nvae_model: madebyollin/sdxl-vae-fp16-fix\nnum_vectors: 4\nplaceholder_token: \"bruce_the_gnome\"\ninitializer_token: \"gnome\"\ncache_vae_outputs: False\ngradient_accumulation_steps: 1\nweight_dtype: bfloat16\ngradient_checkpointing: True\n\nmax_train_steps: 2000\nsave_every_n_steps: 200\nvalidate_every_n_steps: 200\n\nmax_checkpoints: 20\nvalidation_prompts:\n  - A photo of bruce_the_gnome at the beach\n  - A photo of bruce_the_gnome reading a book\ntrain_batch_size: 1\nnum_validation_images_per_prompt: 3\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.type","title":"type","text":"<pre><code>type: Literal[\"SDXL_TEXTUAL_INVERSION\"] = (\n    \"SDXL_TEXTUAL_INVERSION\"\n)\n</code></pre> <p>Must be <code>SDXL_TEXTUAL_INVERSION</code>. This is what differentiates training pipeline types.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.model","title":"model","text":"<pre><code>model: str = 'stabilityai/stable-diffusion-xl-base-1.0'\n</code></pre> <p>Name or path of the base model to train. Can be in diffusers format, or a single stable diffusion checkpoint file. (E.g. 'stabilityai/stable-diffusion-xl-base-1.0', '/path/to/JuggernautXL.safetensors', etc. )</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.hf_variant","title":"hf_variant","text":"<pre><code>hf_variant: str | None = 'fp16'\n</code></pre> <p>The Hugging Face Hub model variant to use. Only applies if <code>model</code> is a Hugging Face Hub model name.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.num_vectors","title":"num_vectors","text":"<pre><code>num_vectors: int = 1\n</code></pre> <p>Note: <code>num_vectors</code> can be overridden by <code>initial_phrase</code>.</p> <p>The number of textual inversion embedding vectors that will be used to learn the concept.</p> <p>Increasing the <code>num_vectors</code> enables the model to learn more complex concepts, but has the following drawbacks:</p> <ul> <li>greater risk of overfitting</li> <li>increased size of the resulting output file</li> <li>consumes more of the prompt capacity at inference time</li> </ul> <p>Typical values for <code>num_vectors</code> are in the range [1, 16].</p> <p>As a rule of thumb, <code>num_vectors</code> can be increased as the size of the dataset increases (without overfitting).</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.placeholder_token","title":"placeholder_token","text":"<pre><code>placeholder_token: str\n</code></pre> <p>The special word to associate the learned embeddings with. Choose a unique token that is unlikely to already exist in the tokenizer's vocabulary.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.initializer_token","title":"initializer_token","text":"<pre><code>initializer_token: str | None = None\n</code></pre> <p>Note: Exactly one of <code>initializer_token</code>, <code>initial_embedding_file</code>, or <code>initial_phrase</code> should be set.</p> <p>A vocabulary token to use as an initializer for the placeholder token. It should be a single word that roughly describes the object or style that you're trying to train on. Must map to a single tokenizer token.</p> <p>For example, if you are training on a dataset of images of your pet dog, a good choice would be <code>dog</code>.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.initial_embedding_file","title":"initial_embedding_file","text":"<pre><code>initial_embedding_file: str | None = None\n</code></pre> <p>Note: Exactly one of <code>initializer_token</code>, <code>initial_embedding_file</code>, or <code>initial_phrase</code> should be set.</p> <p>Path to an existing TI embedding that will be used to initialize the embedding being trained. The placeholder token in the file must match the <code>placeholder_token</code> field.</p> <p>Either <code>initializer_token</code> or <code>initial_embedding_file</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.initial_phrase","title":"initial_phrase","text":"<pre><code>initial_phrase: str | None = None\n</code></pre> <p>Note: Exactly one of <code>initializer_token</code>, <code>initial_embedding_file</code>, or <code>initial_phrase</code> should be set.</p> <p>A phrase that will be used to initialize the placeholder token embedding. The phrase will be tokenized, and the corresponding embeddings will be used to initialize the placeholder tokens. The number of embedding vectors will be inferred from the length of the tokenized phrase, so keep the phrase short. The consequences of training a large number of embedding vectors are discussed in the <code>num_vectors</code> field documentation.</p> <p>For example, if you are training on a dataset of images of pokemon, you might use <code>pokemon sketch white background</code>.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.optimizer","title":"optimizer","text":"<pre><code>optimizer: AdamOptimizerConfig | ProdigyOptimizerConfig = (\n    AdamOptimizerConfig()\n)\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.lr_scheduler","title":"lr_scheduler","text":"<pre><code>lr_scheduler: Literal[\n    \"linear\",\n    \"cosine\",\n    \"cosine_with_restarts\",\n    \"polynomial\",\n    \"constant\",\n    \"constant_with_warmup\",\n] = \"constant\"\n</code></pre>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.lr_warmup_steps","title":"lr_warmup_steps","text":"<pre><code>lr_warmup_steps: int = 0\n</code></pre> <p>The number of warmup steps in the learning rate scheduler. Only applied to schedulers that support warmup. See lr_scheduler.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.min_snr_gamma","title":"min_snr_gamma","text":"<pre><code>min_snr_gamma: float | None = 5.0\n</code></pre> <p>Min-SNR weighting for diffusion training was introduced in https://arxiv.org/abs/2303.09556. This strategy improves the speed of training convergence by adjusting the weight of each sample.</p> <p><code>min_snr_gamma</code> acts like an an upper bound on the weight of samples with low noise levels.</p> <p>If <code>None</code>, then Min-SNR weighting will not be applied. If enabled, the recommended value is <code>min_snr_gamma = 5.0</code>.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.cache_vae_outputs","title":"cache_vae_outputs","text":"<pre><code>cache_vae_outputs: bool = False\n</code></pre> <p>If True, the VAE will be applied to all of the images in the dataset before starting training and the results will be cached to disk. This reduces the VRAM requirements during training (don't have to keep the VAE in VRAM), and speeds up training (don't have to run the VAE encoding step).</p> <p>This option can only be enabled if all non-deterministic image augmentations are disabled (i.e. <code>center_crop=True</code>, <code>random_flip=False</code>, etc.).</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.enable_cpu_offload_during_validation","title":"enable_cpu_offload_during_validation","text":"<pre><code>enable_cpu_offload_during_validation: bool = False\n</code></pre> <p>If True, models will be kept in CPU memory and loaded into GPU memory one-by-one while generating validation images. This reduces VRAM requirements at the cost of slower generation of validation images.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<pre><code>gradient_accumulation_steps: int = 1\n</code></pre> <p>The number of gradient steps to accumulate before each weight update. This is an alternative to increasing the <code>train_batch_size</code> when training with limited VRAM.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.weight_dtype","title":"weight_dtype","text":"<pre><code>weight_dtype: Literal[\"float32\", \"float16\", \"bfloat16\"] = (\n    \"bfloat16\"\n)\n</code></pre> <p>All weights (trainable and fixed) will be cast to this precision. Lower precision dtypes require less VRAM, and result in faster training, but are more prone to issues with numerical stability.</p> <p>Recommendations:</p> <ul> <li><code>\"float32\"</code>: Use this mode if you have plenty of VRAM available.</li> <li><code>\"bfloat16\"</code>: Use this mode if you have limited VRAM and a GPU that supports bfloat16.</li> <li><code>\"float16\"</code>: Use this mode if you have limited VRAM and a GPU that does not support bfloat16.</li> </ul> <p>See also <code>mixed_precision</code>.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.mixed_precision","title":"mixed_precision","text":"<pre><code>mixed_precision: Literal[\"no\", \"fp16\", \"bf16\", \"fp8\"] = \"no\"\n</code></pre> <p>The mixed precision mode to use.</p> <p>If mixed precision is enabled, then all non-trainable parameters will be cast to the specified <code>weight_dtype</code>, and trainable parameters are kept in float32 precision to avoid issues with numerical stability.</p> <p>This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.mixed_precision</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.xformers","title":"xformers","text":"<pre><code>xformers: bool = False\n</code></pre> <p>If <code>True</code>, use xformers for more efficient attention blocks.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.gradient_checkpointing","title":"gradient_checkpointing","text":"<pre><code>gradient_checkpointing: bool = False\n</code></pre> <p>Whether or not to use gradient checkpointing to save memory at the expense of a slower backward pass. Enabling gradient checkpointing slows down training by ~20%.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.max_checkpoints","title":"max_checkpoints","text":"<pre><code>max_checkpoints: int | None = None\n</code></pre> <p>The maximum number of checkpoints to keep. New checkpoints will replace earlier checkpoints to stay under this limit. Note that this limit is applied to 'step' and 'epoch' checkpoints separately.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.prediction_type","title":"prediction_type","text":"<pre><code>prediction_type: (\n    Literal[\"epsilon\", \"v_prediction\"] | None\n) = None\n</code></pre> <p>The prediction type that will be used for training. If <code>None</code>, the prediction type will be inferred from the scheduler.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.max_grad_norm","title":"max_grad_norm","text":"<pre><code>max_grad_norm: float | None = None\n</code></pre> <p>Maximum gradient norm for gradient clipping. Set to <code>None</code> for no clipping.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.validation_prompts","title":"validation_prompts","text":"<pre><code>validation_prompts: list[str] = []\n</code></pre> <p>A list of prompts that will be used to generate images throughout training for the purpose of tracking progress.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.negative_validation_prompts","title":"negative_validation_prompts","text":"<pre><code>negative_validation_prompts: list[str] | None = None\n</code></pre> <p>A list of negative prompts that will be applied when generating validation images. If set, this list should have the same length as 'validation_prompts'.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.num_validation_images_per_prompt","title":"num_validation_images_per_prompt","text":"<pre><code>num_validation_images_per_prompt: int = 4\n</code></pre> <p>The number of validation images to generate for each prompt in <code>validation_prompts</code>. Careful, validation can become very slow if this number is too large.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.train_batch_size","title":"train_batch_size","text":"<pre><code>train_batch_size: int = 4\n</code></pre> <p>The training batch size.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.use_masks","title":"use_masks","text":"<pre><code>use_masks: bool = False\n</code></pre> <p>If True, image masks will be applied to weight the loss during training. The dataset must contain masks for this feature to be used.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.data_loader","title":"data_loader","text":"<pre><code>data_loader: TextualInversionSDDataLoaderConfig\n</code></pre> <p>The data configuration.</p> <p>See <code>TextualInversionSDDataLoaderConfig</code> for details.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.vae_model","title":"vae_model","text":"<pre><code>vae_model: str | None = None\n</code></pre> <p>The name of the Hugging Face Hub VAE model to train against. If set, this will override the VAE bundled with the base model (specified by the <code>model</code> parameter). This config option is provided for SDXL models, because SDXL 1.0 shipped with a VAE that produces NaNs in fp16 mode, so it is common to replace this VAE with a fixed version.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.seed","title":"seed","text":"<pre><code>seed: Optional[int] = None\n</code></pre> <p>A randomization seed for reproducible training. Set to any constant integer for consistent training results. If set to <code>null</code>, training will be non-deterministic.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.base_output_dir","title":"base_output_dir","text":"<pre><code>base_output_dir: str\n</code></pre> <p>The output directory where the training outputs (model checkpoints, logs, intermediate predictions) will be written. A subdirectory will be created with a timestamp for each new training run.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.report_to","title":"report_to","text":"<pre><code>report_to: Literal[\n    \"all\", \"tensorboard\", \"wandb\", \"comet_ml\"\n] = \"tensorboard\"\n</code></pre> <p>The integration to report results and logs to. This value is passed to Hugging Face Accelerate. See <code>accelerate.Accelerator.log_with</code> for more details.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.max_train_steps","title":"max_train_steps","text":"<pre><code>max_train_steps: int | None = None\n</code></pre> <p>Total number of training steps to perform. One training step is one gradient update.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.max_train_epochs","title":"max_train_epochs","text":"<pre><code>max_train_epochs: int | None = None\n</code></pre> <p>Total number of training epochs to perform. One epoch is one pass over the entire dataset.</p> <p>One of <code>max_train_steps</code> or <code>max_train_epochs</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.save_every_n_epochs","title":"save_every_n_epochs","text":"<pre><code>save_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.save_every_n_steps","title":"save_every_n_steps","text":"<pre><code>save_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which to save checkpoints.</p> <p>One of <code>save_every_n_epochs</code> or <code>save_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.validate_every_n_epochs","title":"validate_every_n_epochs","text":"<pre><code>validate_every_n_epochs: int | None = None\n</code></pre> <p>The interval (in epochs) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.validate_every_n_steps","title":"validate_every_n_steps","text":"<pre><code>validate_every_n_steps: int | None = None\n</code></pre> <p>The interval (in steps) at which validation images will be generated.</p> <p>One of <code>validate_every_n_epochs</code> or <code>validate_every_n_steps</code> should be set.</p>"},{"location":"reference/config/pipelines/sdxl_textual_inversion/#invoke_training.pipelines.stable_diffusion_xl.textual_inversion.config.SdxlTextualInversionConfig.check_validation_prompts","title":"check_validation_prompts","text":"<pre><code>check_validation_prompts()\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/","title":"optimizer_config","text":""},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig","title":"AdamOptimizerConfig","text":""},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig.optimizer_type","title":"optimizer_type","text":"<pre><code>optimizer_type: Literal['AdamW'] = 'AdamW'\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig.learning_rate","title":"learning_rate","text":"<pre><code>learning_rate: float = 0.0001\n</code></pre> <p>Initial learning rate to use (after the potential warmup period). Note that in some training pipelines this can be overriden for a specific group of params: https://pytorch.org/docs/stable/optim.html#per-parameter-options (E.g. see <code>text_encoder_learning_rate</code> and <code>unet_learning_rate</code>)</p>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig.beta1","title":"beta1","text":"<pre><code>beta1: float = 0.9\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig.beta2","title":"beta2","text":"<pre><code>beta2: float = 0.999\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig.weight_decay","title":"weight_decay","text":"<pre><code>weight_decay: float = 0.01\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig.epsilon","title":"epsilon","text":"<pre><code>epsilon: float = 1e-08\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.AdamOptimizerConfig.use_8bit","title":"use_8bit","text":"<pre><code>use_8bit: bool = False\n</code></pre> <p>Use an 8-bit version of the Adam optimizer. This requires the bitsandbytes library to be installed. use_8bit reduces the VRAM usage of the optimizer, but increases the risk of issues with numerical stability.</p>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.ProdigyOptimizerConfig","title":"ProdigyOptimizerConfig","text":""},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.ProdigyOptimizerConfig.optimizer_type","title":"optimizer_type","text":"<pre><code>optimizer_type: Literal['Prodigy'] = 'Prodigy'\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.ProdigyOptimizerConfig.learning_rate","title":"learning_rate","text":"<pre><code>learning_rate: float = 1.0\n</code></pre> <p>The learning rate. For the Prodigy optimizer, the learning rate is adjusted dynamically. A value of 1.0 is recommended. Note that in some training pipelines this can be overriden for a specific group of params: https://pytorch.org/docs/stable/optim.html#per-parameter-options (E.g. see <code>text_encoder_learning_rate</code> and <code>unet_learning_rate</code>)</p>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.ProdigyOptimizerConfig.weight_decay","title":"weight_decay","text":"<pre><code>weight_decay: float = 0.0\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.ProdigyOptimizerConfig.use_bias_correction","title":"use_bias_correction","text":"<pre><code>use_bias_correction: bool = False\n</code></pre>"},{"location":"reference/config/shared/optimizer_config/#invoke_training.config.optimizer.optimizer_config.ProdigyOptimizerConfig.safeguard_warmup","title":"safeguard_warmup","text":"<pre><code>safeguard_warmup: bool = False\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/","title":"data_loader_config","text":""},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.AspectRatioBucketConfig","title":"AspectRatioBucketConfig","text":""},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.AspectRatioBucketConfig.target_resolution","title":"target_resolution","text":"<pre><code>target_resolution: int\n</code></pre> <p>The target resolution for all aspect ratios. When generating aspect ratio buckets, the resolution of each bucket is selected to have roughly <code>target_resolution * target_resolution</code> pixels (i.e. a square image with dimensions equal to <code>target_resolution</code>).</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.AspectRatioBucketConfig.start_dim","title":"start_dim","text":"<pre><code>start_dim: int\n</code></pre> <p>Aspect ratio bucket resolutions are generated as follows:</p> <ul> <li>Iterate over 'first' dimension values from <code>start_dim</code> to <code>end_dim</code> in steps of size <code>divisible_by</code>.</li> <li>Calculate the 'second' dimension to be as close as possible to the total number of pixels in <code>target_resolution</code>, while still being divisible by <code>divisible_by</code>.</li> </ul> Choosing aspect ratio buckets <p>The aspect ratio bucket resolutions are logged at the start of training with the number of images in each bucket. Review these logs to make sure that images are being split into buckets as expected.</p> <p>Highly fragmented splits (i.e. many buckets with few examples in each) can 1) limit the extent to which examples can be shuffled, and 2) slow down training if there are many partial batches.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.AspectRatioBucketConfig.end_dim","title":"end_dim","text":"<pre><code>end_dim: int\n</code></pre> <p>See explanation under <code>start_dim</code>.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.AspectRatioBucketConfig.divisible_by","title":"divisible_by","text":"<pre><code>divisible_by: int\n</code></pre> <p>See explanation under <code>start_dim</code>.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig","title":"ImageCaptionSDDataLoaderConfig","text":""},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.type","title":"type","text":"<pre><code>type: Literal[\"IMAGE_CAPTION_SD_DATA_LOADER\"] = (\n    \"IMAGE_CAPTION_SD_DATA_LOADER\"\n)\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.dataset","title":"dataset","text":"<pre><code>dataset: ImageCaptionDatasetConfig\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.aspect_ratio_buckets","title":"aspect_ratio_buckets","text":"<pre><code>aspect_ratio_buckets: AspectRatioBucketConfig | None = None\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.resolution","title":"resolution","text":"<pre><code>resolution: int | tuple[int, int] = 512\n</code></pre> <p>The resolution for input images. Either a scalar integer representing the square resolution height and width, or a (height, width) tuple. All of the images in the dataset will be resized to this resolution unless the <code>aspect_ratio_buckets</code> config is set.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.center_crop","title":"center_crop","text":"<pre><code>center_crop: bool = True\n</code></pre> <p>If True, input images will be center-cropped to the target resolution. If False, input images will be randomly cropped to the target resolution.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.random_flip","title":"random_flip","text":"<pre><code>random_flip: bool = False\n</code></pre> <p>Whether random flip augmentations should be applied to input images.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.caption_prefix","title":"caption_prefix","text":"<pre><code>caption_prefix: str | None = None\n</code></pre> <p>A prefix that will be prepended to all captions. If None, no prefix will be added.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionSDDataLoaderConfig.dataloader_num_workers","title":"dataloader_num_workers","text":"<pre><code>dataloader_num_workers: int = 0\n</code></pre> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig","title":"ImageCaptionFluxDataLoaderConfig","text":""},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.type","title":"type","text":"<pre><code>type: Literal[\"IMAGE_CAPTION_FLUX_DATA_LOADER\"] = (\n    \"IMAGE_CAPTION_FLUX_DATA_LOADER\"\n)\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.dataset","title":"dataset","text":"<pre><code>dataset: ImageCaptionDatasetConfig\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.aspect_ratio_buckets","title":"aspect_ratio_buckets","text":"<pre><code>aspect_ratio_buckets: AspectRatioBucketConfig | None = None\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.resolution","title":"resolution","text":"<pre><code>resolution: int | tuple[int, int] = 512\n</code></pre> <p>The resolution for input images. Either a scalar integer representing the square resolution height and width, or a (height, width) tuple. All of the images in the dataset will be resized to this resolution unless the <code>aspect_ratio_buckets</code> config is set.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.center_crop","title":"center_crop","text":"<pre><code>center_crop: bool = True\n</code></pre> <p>If True, input images will be center-cropped to the target resolution. If False, input images will be randomly cropped to the target resolution.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.random_flip","title":"random_flip","text":"<pre><code>random_flip: bool = False\n</code></pre> <p>Whether random flip augmentations should be applied to input images.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.caption_prefix","title":"caption_prefix","text":"<pre><code>caption_prefix: str | None = None\n</code></pre> <p>A prefix that will be prepended to all captions. If None, no prefix will be added.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.ImageCaptionFluxDataLoaderConfig.dataloader_num_workers","title":"dataloader_num_workers","text":"<pre><code>dataloader_num_workers: int = 0\n</code></pre> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig","title":"DreamboothSDDataLoaderConfig","text":""},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.type","title":"type","text":"<pre><code>type: Literal[\"DREAMBOOTH_SD_DATA_LOADER\"] = (\n    \"DREAMBOOTH_SD_DATA_LOADER\"\n)\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.instance_caption","title":"instance_caption","text":"<pre><code>instance_caption: str\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.class_caption","title":"class_caption","text":"<pre><code>class_caption: Optional[str] = None\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.instance_dataset","title":"instance_dataset","text":"<pre><code>instance_dataset: ImageDirDatasetConfig\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.class_dataset","title":"class_dataset","text":"<pre><code>class_dataset: Optional[ImageDirDatasetConfig] = None\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.class_data_loss_weight","title":"class_data_loss_weight","text":"<pre><code>class_data_loss_weight: float = 1.0\n</code></pre> <p>The loss weight applied to class dataset examples. Instance dataset examples have an implicit loss weight of 1.0.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.aspect_ratio_buckets","title":"aspect_ratio_buckets","text":"<pre><code>aspect_ratio_buckets: AspectRatioBucketConfig | None = None\n</code></pre> <p>The aspect ratio bucketing configuration. If None, aspect ratio bucketing is disabled, and all images will be resized to the same resolution.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.resolution","title":"resolution","text":"<pre><code>resolution: int | tuple[int, int] = 512\n</code></pre> <p>The resolution for input images. Either a scalar integer representing the square resolution height and width, or a (height, width) tuple. All of the images in the dataset will be resized to this resolution unless the <code>aspect_ratio_buckets</code> config is set.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.center_crop","title":"center_crop","text":"<pre><code>center_crop: bool = True\n</code></pre> <p>If True, input images will be center-cropped to the target resolution. If False, input images will be randomly cropped to the target resolution.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.random_flip","title":"random_flip","text":"<pre><code>random_flip: bool = False\n</code></pre> <p>Whether random flip augmentations should be applied to input images.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.DreamboothSDDataLoaderConfig.dataloader_num_workers","title":"dataloader_num_workers","text":"<pre><code>dataloader_num_workers: int = 0\n</code></pre> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig","title":"TextualInversionSDDataLoaderConfig","text":""},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.type","title":"type","text":"<pre><code>type: Literal[\"TEXTUAL_INVERSION_SD_DATA_LOADER\"] = (\n    \"TEXTUAL_INVERSION_SD_DATA_LOADER\"\n)\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.dataset","title":"dataset","text":"<pre><code>dataset: ImageDirDatasetConfig | ImageCaptionDatasetConfig\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.caption_preset","title":"caption_preset","text":"<pre><code>caption_preset: Literal['style', 'object'] | None = None\n</code></pre>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.caption_templates","title":"caption_templates","text":"<pre><code>caption_templates: list[str] | None = None\n</code></pre> <p>A list of caption templates with a single template argument 'slot' in each. E.g.:</p> <ul> <li>\"a photo of a {}\"</li> <li>\"a rendering of a {}\"</li> <li>\"a cropped photo of the {}\"</li> </ul>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.keep_original_captions","title":"keep_original_captions","text":"<pre><code>keep_original_captions: bool = False\n</code></pre> <p>If <code>True</code>, then the captions generated as a result of the <code>caption_preset</code> or <code>caption_templates</code> will be used as prefixes for the original captions. If <code>False</code>, then the generated captions will replace the original captions.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.aspect_ratio_buckets","title":"aspect_ratio_buckets","text":"<pre><code>aspect_ratio_buckets: AspectRatioBucketConfig | None = None\n</code></pre> <p>The aspect ratio bucketing configuration. If None, aspect ratio bucketing is disabled, and all images will be resized to the same resolution.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.resolution","title":"resolution","text":"<pre><code>resolution: int | tuple[int, int] = 512\n</code></pre> <p>The resolution for input images. Either a scalar integer representing the square resolution height and width, or a (height, width) tuple. All of the images in the dataset will be resized to this resolution unless the <code>aspect_ratio_buckets</code> config is set.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.center_crop","title":"center_crop","text":"<pre><code>center_crop: bool = True\n</code></pre> <p>If True, input images will be center-cropped to the target resolution. If False, input images will be randomly cropped to the target resolution.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.random_flip","title":"random_flip","text":"<pre><code>random_flip: bool = False\n</code></pre> <p>Whether random flip augmentations should be applied to input images.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.shuffle_caption_delimiter","title":"shuffle_caption_delimiter","text":"<pre><code>shuffle_caption_delimiter: str | None = None\n</code></pre> <p>If <code>None</code>, then no caption shuffling is applied. If set, then captions are split on this delimiter and shuffled.</p>"},{"location":"reference/config/shared/data/data_loader_config/#invoke_training.config.data.data_loader_config.TextualInversionSDDataLoaderConfig.dataloader_num_workers","title":"dataloader_num_workers","text":"<pre><code>dataloader_num_workers: int = 0\n</code></pre> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</p>"},{"location":"reference/config/shared/data/dataset_config/","title":"dataset_config","text":""},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionDatasetConfig","title":"ImageCaptionDatasetConfig","text":"<pre><code>ImageCaptionDatasetConfig = Annotated[\n    Union[\n        HFHubImageCaptionDatasetConfig,\n        ImageCaptionJsonlDatasetConfig,\n        ImageCaptionDirDatasetConfig,\n    ],\n    Field(discriminator=\"type\"),\n]\n</code></pre>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.HFHubImageCaptionDatasetConfig","title":"HFHubImageCaptionDatasetConfig","text":""},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.HFHubImageCaptionDatasetConfig.type","title":"type","text":"<pre><code>type: Literal[\"HF_HUB_IMAGE_CAPTION_DATASET\"] = (\n    \"HF_HUB_IMAGE_CAPTION_DATASET\"\n)\n</code></pre>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.HFHubImageCaptionDatasetConfig.dataset_name","title":"dataset_name","text":"<pre><code>dataset_name: str\n</code></pre> <p>The name of a Hugging Face dataset.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.HFHubImageCaptionDatasetConfig.dataset_config_name","title":"dataset_config_name","text":"<pre><code>dataset_config_name: Optional[str] = None\n</code></pre> <p>The Hugging Face dataset config name. Leave as None if there's only one config.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.HFHubImageCaptionDatasetConfig.hf_cache_dir","title":"hf_cache_dir","text":"<pre><code>hf_cache_dir: Optional[str] = None\n</code></pre> <p>The Hugging Face cache directory to use for dataset downloads. If None, the default value will be used (usually '~/.cache/huggingface/datasets').</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.HFHubImageCaptionDatasetConfig.image_column","title":"image_column","text":"<pre><code>image_column: str = 'image'\n</code></pre> <p>The name of the dataset column that contains image paths.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.HFHubImageCaptionDatasetConfig.caption_column","title":"caption_column","text":"<pre><code>caption_column: str = 'text'\n</code></pre> <p>The name of the dataset column that contains captions.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionJsonlDatasetConfig","title":"ImageCaptionJsonlDatasetConfig","text":""},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionJsonlDatasetConfig.type","title":"type","text":"<pre><code>type: Literal[\"IMAGE_CAPTION_JSONL_DATASET\"] = (\n    \"IMAGE_CAPTION_JSONL_DATASET\"\n)\n</code></pre>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionJsonlDatasetConfig.jsonl_path","title":"jsonl_path","text":"<pre><code>jsonl_path: str\n</code></pre> <p>The path to a JSONL file containing image paths and captions.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionJsonlDatasetConfig.image_column","title":"image_column","text":"<pre><code>image_column: str = 'image'\n</code></pre> <p>The name of the dataset column that contains image paths.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionJsonlDatasetConfig.caption_column","title":"caption_column","text":"<pre><code>caption_column: str = 'text'\n</code></pre> <p>The name of the dataset column that contains captions.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionJsonlDatasetConfig.keep_in_memory","title":"keep_in_memory","text":"<pre><code>keep_in_memory: bool = False\n</code></pre> <p>If <code>True</code>, load all images into memory on initialization so that they can be accessed quickly. If <code>False</code>, images are loaded from disk each time they are accessed. Setting to <code>True</code> improves performance for datasets that are small enough to be kept in memory.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageDirDatasetConfig","title":"ImageDirDatasetConfig","text":""},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageDirDatasetConfig.type","title":"type","text":"<pre><code>type: Literal['IMAGE_DIR_DATASET'] = 'IMAGE_DIR_DATASET'\n</code></pre>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageDirDatasetConfig.dataset_dir","title":"dataset_dir","text":"<pre><code>dataset_dir: str\n</code></pre> <p>The directory to load images from.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageDirDatasetConfig.keep_in_memory","title":"keep_in_memory","text":"<pre><code>keep_in_memory: bool = False\n</code></pre> <p>If <code>True</code>, load all images into memory on initialization so that they can be accessed quickly. If <code>False</code>, images are loaded from disk each time they are accessed. Setting to <code>True</code> improves performance for datasets that are small enough to be kept in memory.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionDirDatasetConfig","title":"ImageCaptionDirDatasetConfig","text":""},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionDirDatasetConfig.type","title":"type","text":"<pre><code>type: Literal[\"IMAGE_CAPTION_DIR_DATASET\"] = (\n    \"IMAGE_CAPTION_DIR_DATASET\"\n)\n</code></pre>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionDirDatasetConfig.dataset_dir","title":"dataset_dir","text":"<pre><code>dataset_dir: str\n</code></pre> <p>The directory to load images from.</p>"},{"location":"reference/config/shared/data/dataset_config/#invoke_training.config.data.dataset_config.ImageCaptionDirDatasetConfig.keep_in_memory","title":"keep_in_memory","text":"<pre><code>keep_in_memory: bool = False\n</code></pre> <p>If <code>True</code>, load all images into memory on initialization so that they can be accessed quickly. If <code>False</code>, images are loaded from disk each time they are accessed. Setting to <code>True</code> improves performance for datasets that are small enough to be kept in memory.</p>"}]}